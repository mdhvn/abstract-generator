Title:  PreCog: Improving Crowdsourced Data Quality

Maximum word frequency:  the ( 592 )
====================
To this end, we present Precog1 , a crowdsourced data acquisition system that supports pre-hoc quality control for both simple data types and multi-paragraph text attributes.

15.7048380693

====================
Although crowdsourcing is used to collect labels and simple data for machine learning applications, many popular online communities such as Amazon, AirBnB, Quora, Reddit, and others also rely on collecting and presenting high quality, open-ended content that is crowdsourced from their users.

12.9907097709

====================
A dominant use case for crowdsourcing is to collect data-- labels, opinions, text extraction, ratings--from large groups of workers.

12.4346377554

====================
INTRODUCTION Figure 2: Text acquisition with pre-hoc (beige background) and post-hoc quality control.

12.1580820267

====================
In this paper, we argue for pre-hoc quality control systems.

11.640864611

====================
The only change is the beige component, which augments the data collection interface (task interface) to estimate the quality of the user's (in this case) text, and automatically provide feedback if the predicted quality is low.

11.5902459912

====================
For example, Amazon crowdsources product reviews by asking customers to rate products and write reviews for them; rental services (e.g., AirbnB) relies on rental hosts to describe their rental properties in quantitative (e.g., number of bed rooms, wireless) as well as qualitative terms (e.g., textual description).

11.1732757445

====================
To build high-quality models, we survey and categorize features from the writing analysis literature into 5 categories  1. user input  2. segment by topic  3. estimate quality  4. targeted feedback  Figure 3: The Segment-Predict-Explain pattern: Precog splits user input into coherent segments; estimates the quality of each segment and the text as a whole; and generates and shows suggested improvements to the user.

10.7154072281

====================
To this end, we performed a survey of literature spanning of social media text analysis [32, 55, 57, 82, 85], essay grading [4,25,58,89], deception detection [23,60], and information retrieval [64, 84].

10.6025327283

====================
For example, task replication [43, 80] assigns the same task to multiple workers and aggregates them into a single result; multi-stage workflow design [6, 48] uses additional crowd tasks to (iteratively) refine previously submitted tasks; in text acquisition, filtering/ranking [1, 37, 67, 82, 84, 86, 90, 95] uses crowd tasks to assess each document's quality and either rank them by quality or filter out low-quality documents.

10.2068512947

====================
In this section, we first describe how users express Precog quality control for common data integrity constraints, as well as quality scores on a crowd-sourced table.

9.72334363294

====================
We further show that Precog's unique approach to combining prescriptive explanations and segment-level feedback improves text quality by 14.3%, and over 3x better than a state-of-the-art feedback system [50].

9.58324289821

====================
Our experiments and prior work [52] show that this is less effective than a more customized approach.

9.56385321011

====================
An alternative is to use existing model explanation algorithms [76] to describe the prediction.

9.50151227474

====================
Our efficient solution called TCruise leverages the structure of random forest models to generate explanations in interactive time.

9.14793404288

====================
In addition to evaluating Precog for hard constraints and simple data types, we evaluate Precog's text feedback through extensive MTurk experiments on two real application domains--product reviews and rental host profiles.

9.10223787377

====================
Precog takes long form text from a crowd worker, decomposes it into coherent portions (segments) based on their topics, predicts the quality of each segment, and automatically generates immediate feedback to explain how these segments can be improved.

9.06273521501

====================
As illustrated in Figure 3, we employ a novel SegmentPredict-Explain pattern to generate customized feedback on an individual segment (rather than document) level.

9.03207062468

====================
For this, we use TopicTiling [77], a sliding window-based segmentation algorithm that computes the dominant topics within the window using LDA [8].

8.97204733824

====================
* Extensive MTurk experiments on two real-world domains with different quality measures: helpfulness for Amazon product reviews and trustworthiness for AirBnB housing profiles.

8.87727292673

====================
To this end, we use a technique called TopicTiling [77], an extension to TextTiling [40].

8.85246067966

====================
We then randomly assigned each worker 50 segments to label, and collected labels until each segment had >= 3 labels, and determined the final label of each segment using the Get Another Label algorithm [81].

8.8392636995

====================
Quality control for crowdsourcing has been extensively studied [54] and can be modeled in two phases.

8.78920545676

====================
Figure 1 illustrates a typical text acquisition workflow: the crowd generates text documents, more tasks are used to estimate the text quality, low-quality documents are removed, and this may ultimately trigger the need to collect more data.

8.73050865389

====================
The basic idea is to push data-quality constraints down to the data collection interface rather than validate them after data acquisition.

8.68969642558

====================
count) LDA topic distribution and top topics [8], entropy across topic distribution opinion sentence distribution stats [64], valence, polarity, and subjectivity scores and distribution across sentences [32, 34, 56], % upper case characters, first person usage, adjectives spelling errors [45], ARI, Gunning index, Coleman-Liau index, Flesch Reading tests, SMOG, punctuation, parts of speech distribution, lexical diversity measures, LIWC grammar features various TF-IDF and top parts of speech comparisons with sample of low and high utility documents  Table 1: Summary of feature library for text quality.

8.68931642066

====================
For instance, Amazon already has a corpus of high and low-quality reviews, and similarly for other applications.

8.67163675779

====================
We first introduce the Prescriptive Explanation problem, which assigns responsibility to each model feature proportional to the amount that it will contribute to improving the predicted text quality.

8.6696075372

====================
We then computed pairwise accuracies between the document labels, classifier predictions, and crowd labels: 71.1% (Classifier predicting Crowd Label), 72.5% (Classifier predicting Document label), and 69.5% (Document label predicting Crowd Label).

8.59085935199

====================
To this end, we define the responsibility Sfdi of a feature fi for input point d as the sum of all maximum influence perturbations that involve fi (e.g., the perturbation pi 6= 0): X  Sfdi =  I(d, p)  I(d, qij ) =  p[?

8.5761732093

====================
Precog also achieves 79% accuracy at predicting if an Airbnb profile is above or below median trustworthiness, using trustworthiness data from [94].

8.54436156463

====================
Although developers can easily implement their own FEFs, Precog is pre-populated with 5 FEFs that work across the two application domains used for evaluation.

8.42955524608

====================
Product-Review Participants: For the laptop review experiment, we recruited 85 workers on Amazon's Mechanical Turk (61.2% male, 38.8% female, ages 20-65 uage =32, sage =8.5).

8.33331731396

====================
To do so, we develop a novel perturbation-based analysis to identify the combination of features that, when changed, will most likely reclassify the text as high quality.

8.2888872357

====================
The final FEATURE table review_feats is used in the later sections to represent the features extracted from the value of the primary key (review).

8.28857595699

====================
We believe our assumption about the availability of a training corpus is reasonable in data acquisition settings, because such quality labels are already gathered in order to rank documents (e.g., Amazon helpful/unhelpful reviews, Reddit comment up/down votes).

8.28334872191

====================
Companies (e.g., Amazon, Zappos) use this post-hoc technique by asking users to assess whether a product review is "helpful" or "not helpful", and ranks and displays reviews based on this measure.

8.26376585625

====================
CREATE CROWD TABLE users ( id autoincrement primary key, username text UNIQUE, age int CHECK age > 0 AND age < 100, CHECK(username matches \w+) ); CREATE CROWD TABLE reviews( id autoincrement primary key, product_id text, rating int CHECK rating > 0 AND rating <= 5, review text, QUALITY SCORE qualreview qual_udf(review), FOREIGN KEY product_id REF products(id) ); CREATE FEATURE TABLE review_feats( review text primary key references reviews.review, topics FEATURE topic_extractor, len FEATURE len_extracton, ... );  Generic Feedback: Precog automatically generates feedback based on the error message that the underlying database generates when the INSERT violates a constraint.

8.24160016801

====================
The fourth FEF for product reviews was mapped to Subjectivity features in (Table 1) and the fourth host profiles FEF was mapped to Friendliness LIWC features shown in [94], with each returning text suggesting that the user improves the respective facet of their submission (i.e., "Please make your writing more balanced and neutral").

8.21476591194

====================
foxtype.com/, 2016.

8.19164711388

====================
We define this as the Prescriptive Explanation problem, and find that the search space of solutions for the problem is exponential in the number of model features.

8.17764150157

====================
Pre-hoc methods improve quality before the data is acquired (submitted); Post-hoc methods improve quality after data acquisition (i.e., after submission).

8.14304219039

====================
, qik ; each path qij matches a subset of the training dataset Dij [?]

8.13541182329

====================
Most studies focus on post-hoc quality control, often using additional crowdsourced tasks to assess and improve the quality.

8.02685015622

====================
5.2  Figure 6: Assigning responsibility to perturbations.

7.99174514786

====================
Existing approaches (surveyed in Related Work) focus on syntactic errors such as grammatical mistakes, which cannot help improve the text content, or overly simple models for picking feedback text [50].

7.97094260675

====================
The first category, Informativeness, highlights trends across existing literature that show that both general length measures [4, 50, 55, 57, 82] as well as domain-specific jargon are highly predictive of quality [55, 57, 60].

7.96339592251

====================
In contrast, we generate prescriptive, actionable explanations that, if followed, are expected to improve the text.

7.95226913232

====================
We address the former challenge using a data-driven approach that learns a quality measure from data that has already been acquired.

7.93461838513

====================
For quantitative attributes, a common dataquality constraint is to ensure values are not out of bounds (e.g., human age should be above 0).

7.93297762628

====================
Our contribution is to curate the subset of these features that can be generalized across text domains to improve writing quality, categorize them (Table 1), and integrate them into an open source feature library4 .

7.93278287471

====================
Precog, which is complementary to post-hoc quality control techniques, collects >= 2x high-quality documents for the same budget as no feedback, and improves text quality by 14.3% on average.

7.90381307176

====================
For explanation functions, prior work showed that 75% of reasons for unhelpful reviews were covered by (in priority order) overly emotional/biased opinions, lack of information/not enough detail, irrelevant comments, and poor writing style [18].

7.86584059392

====================
We then map these feature combinations to explanation functions that are executed to generate the final set of feedback text (Section 5).

7.85605873753

====================
2.1  Pushing Data Constraints to the Interface  Precog extends existing crowdsourced databases that contain crowdsourced and non-crowdsourced base relations; a crowdsourced table [28] represents a subset of all possible records that may be stored in the table and the task is to acquire records to insert into the table.

7.81340513012

====================
Although it's possible to automatically perform pre-hoc quality control for simple constraints over simple data types, it is still unclear how this can be achieved for more complex data integrity constraints and data types.

7.78542028843

====================
Furthermore, while most approaches simply use the distribution of topics as a feature [57, 62], Precog computes several summary statistics (entropy, topic ID and probability of top-K topics, ranked by probability) not used in prior work that prove highly predictive in our experiments.

7.77406693124

====================
We then synthesized existing research to write 4 explanation functions for each domain, with 3 overlapping between the two.

7.74213249361

====================
Segmentation: Contributor rubrics across many social media services are structured around topics [2, 92, 96], and psychology research suggests that mentally processing the topical hierarchy of text is fundamental to the reading process [41].

7.71777857985

====================
Overview: In contrast to naive form validation, which simply rejects user inputs with an error message, Precog seeks to accommodate iterative improvements through feedback interfaces.

7.71183631134

====================
The backend splits the review into coherent segments, identifies the low-quality segments, and generates document-level feedback.

7.68133612422

====================
For instance, multi-paragraph text attributes such as product reviews, forum comments, or rental descriptions are particularly challenging for several reasons.

7.6762608085

====================
(e.g., readability, informativeness, etc), and implement a representative and extensible library of 47 text quality features.

7.66957644649

====================
This can be achieved by dynamically identifying these constraint violations and providing feedback to the user.

7.6637011309

====================
We then use explanation functions to transform the most responsible features into prescriptive feedback for the user.

7.65494254377

====================
1  In fact, instances of pre-hoc quality control are already commonly used in practice, both in the survey design literature [36] and as form design throughout the Internet.

7.63701160137

====================
Readability/Grammar is an aggregate of syntactic features shown predictive across multiple domains [23, 50, 60, 82].

7.62764722674

====================
We then performed pairwise Tukey HSD post-hoc tests between each of the four conditions.

7.61252162118

====================
Pre-hoc quality control occurs before data acquisition and naturally complements many existing post-hoc techniques to further improve the final data quality.

7.59724042949

====================
By default we use this library for learning quality measures from a corpus.

7.57004734782

====================
Since good feedback can help the worker improve the text, it naturally improves the quality of the acquired data, and can reduce data acquisition costs.

7.56409885407

====================
In isolation, existing approaches may find that the length is large and suggest reducing it, and that the emotion is high and suggest reducing it.

7.54866062577

====================
We validated generalizability of the model to domains not covered in prior work by evaluating it on reddit comments from the AskScience subreddit5 and predicted comment helpfulness on an evenly balanced sample with 80% accuracy6 .

7.54095937367

====================
Crowd-based feedback is effective, but can take 20 minutes to generate feedback [52] and are essentially posthoc because they create new crowd tasks to refine previously submitted ones.

7.53208380715

====================
We then normalize a feature's responsibility Sfdi by computing Snormdfi =  d Sf -uf  i  i  sf  .

7.52515543129

====================
]Rn  P(d) =  [  Rather than examining all possible perturbations, our heuristic to compute Sfdi restricts the set of perturbations with respect to the decision paths in the trees that increase d's utility.

7.52172233672

====================
As compared to other features libraries such as LIWC, Precog's main advantage is a high-concentration of data-driven features (topic modeling,  4  5  Available at http://cudbg.github.io/Dialectic  Category Informativeness Topic  # 8 5  Subjectivity  15  Readability and Grammar  15  Similarity  4  Description mined jargon word and named entity stats [64], length measures (word, sentence, etc.

7.51176750785

====================
Automated approaches such as auto-graders primarily focus on predicting quality rather than generating feedback [4, 25, 58, 89]; others are limited to syntactic analysis [27, 35, 63], or generate overly simple writing feedback [7, 10, 49].

7.50026780952

====================
These statements complement existing task interface specifications that prior crowdsourcing systems [28, 59, 71] use for task generation by providing a way to augment them for data integrity constraints.

7.49304502319

====================
All participants were US Residents with > 90% HIT accept rates.

7.46754174674

====================
A detailed explanation of the four conditions is shown in Section 7.2.2.

7.45906548848

====================
Protocol and Rubric for Assessing Quality: Three independent evaluators (non-authors) coded the pre and post-feedback documents using a rubric based on prior work on review quality [18, 55, 67] and Airbnb profile quality [57].

7.44684083887

====================
def notEnoughDetail(topics, featureCnt, textLen, text="", feats=[]): if featureCnt < 10 and textLen < threshold: return "Try adding information about: " + suggest_new_prod_feats(topics, text, feats) ...  We note that existing feedback systems [7,49,50] implicitly follow this model, however they bind individual features to static strings.

7.4441189051

====================
A developer first defines an explanation function that takes as input the list of attribute names and values for which the constraint is defined for (in order to support multi-attribute constraints) and the error message, and returns a string that is shown as feedback.

7.44186575087

====================
Second, it is ill-defined and applicationdependent, thus difficult to specify as a constraint.

7.4398215664

====================
REFERENCES  [25] N. Farra, S. Somasundaran, and J. Burstein.

7.42666276492

====================
We describe our experiments that show that it is possible to use these labels as a proxy for individual segments.

7.42540584148

====================
We first describe our extensible feature library that consolidates text features across literature in social media text analysis, essay grading, language psychology, and data mining research communities.

7.41421192072

====================
The review rubric asks coders to scores reviews on helpfulness to laptop shoppers, and the host profile rubric asks coders to score profiles based on trustworthiness to potential tenants.

7.41416687808

====================
Rather than define a concrete quality measure, Precog automatically learns the quality measure from a training corpus that contains documents along with their quality labels (for the entire document, not each segment).

7.41012023318

====================
We then sort the FEFs by their average scores and take the top k with a score above the threshold t.  7.

7.38400043371

====================
Precog: A PRECOG SYSTEM  As described in the introduction, Precog seeks to optimize the data collection interface in order to improve the quality of the collected data and ensure data quality constraints.

7.38137691985

====================
We then select the maximal  No feedback needed if data point already has high utility.

7.38099089281

====================
The left column shows the feedback interface generated by default.

7.3803642615

====================
The paths go from the document's current low quality classification to a high quality classification.

7.35265083158

====================
Each rubric rated documents on a 1-7 Likert scale using three specific aspects identified by prior work--Informativity, Subjectivity, Readability--for reviews--Ability, Benevolence, Integrity-for profile trustworthiness, as well as a holistic overall score.

7.35213435227

====================
For instance, Amazon product reviews and users may be modeled using the following crowdbased DDL statements.

7.34962368413

====================
Purple arrows show the feedback process for hard constraints.

7.3307551276

====================
To summarize our contributions: * We present the argument for pre-hoc quality control and present its unique advantages as well as the challenges for multi-paragraph text.

7.32232939945

====================
The prior work predicts the quality of Amazon DVD, AV player and Camera reviews with 83% accuracy; Precog's default model on the same setup predicts at 85% accuracy--the slight improvement is due to the additional features in the topic and similarity categories from other literature (Table 1).

7.31555860509

====================
We then gave participants the opportunity to revise their submission; to avoid bias, we noted that they were not obligated to.

7.31507762324

====================
However, recent study shows the promise of translating semantic features to textual feedback [50].

7.30410443431

====================
Our goal is to provide the foundation for such content-specific semantic feedback by surveying and categorizing features from the writing analysis literature.

7.29028851425

====================
Figure 4 summarizes Precog into three levels based on the amount of customization needed by the developer.

7.28718873124

====================
def exp_func(att1, val1, ..., attn, valn, err=None): return "custom error message" CREATE EXPLANATION <func> ON <table>(<att1>,..<attn>) FOR <CONSTRAINT NAME> USING <explanation function>;  Below is the specification to customize the feedback for a numeric domain constraint3 .

7.2824764007

====================
Experimental Conditions: The purpose of experiments is to both show the Cost Saving benefits of Precog as well as to evaluate the effectiveness of it's two main features (segment-level feedback and TCruise explanation generation).

7.28144928631

====================
By default, Precog provides optimizations for constraints over numerical and categorical data types, and can be extended with custom optimizations.

7.28040932858

====================
The core challenges are to (1) identify a proxy for text quality that is consistent with the downstream application's needs, and (2) to generate effective feedback text.

7.27675239795

====================
NEW APPLICATION DOMAINS  EXPERIMENTS  We now evaluate how Precog improves high-quality data acquisition using live Mechanical Turk deployments.

7.27225089865

====================
Below, we describe how developers can express the three levels of Precog quality control for domain, foreign-key, uniqueness, and quality score constraints in Figure 4.

7.26897486566

====================
[28] M. J. Franklin, D. Kossmann, T. Kraska, S. Ramesh, and R. Xin.

7.26518385637

====================
In this work, we restrict the analysis to perturbations that have the maximal influence.

7.25992976193

====================
Third, it's unclear how to automatically generate the appropriate feedback text to show the user.

7.24450601682

====================
This provides the functionality for our automatic pre-hoc quality control system for free-form text attributes.

7.24285022075

====================
Our design is informed by the writing analysis and feedback literature, which emphasizes the value of providing immediate feedback [51], as well as finegrained feedback for specific portions of the text [17, 78, 83], as is common in coding environments.

7.24236117327

====================
Krause [50] was shown to outperform static explanations of important components of a helpful review (similar to a rubric) for students performing peer code-reviews and uses an outlier based approach described in Section 5.1.

7.23701855007

====================
Subjectivity assesses user bias using a variety of features ranging from sentiment analysis [32, 34, 56] to pronoun usage [73].

7.23534287408

====================
Although there might be a number of segments mislabeled, the model can tolerate their impact well and achieve good performance.

7.2278766049

====================
CONCLUSION AND FUTURE WORK  This paper presented the design, implementation and evaluation of Precog, a pre-hoc quality control system.

7.21043665243

====================
Fp is the subset of features that p perturbs:  Sfd  We are now ready to present the key technical problem for text acquisition feedback:  5.4  U (vij ) - U (M (d))  Our implementation indexes all paths in the random forest by their utility.

7.20499735777

====================
Each worker was randomly assigned to one of three conditions, one for each of the interfaces shown in Figure 7.

7.18385170342

====================
For hard constraints (Purple), user inputs are sent to the  4  jargon usage, text similarity measures) that are trained to fit each developer's unique corpus.

7.18306321639

====================
The primary features that we do not include are those that rely on application metadata such as the worker's history or location, which may be predictive of quality but not related to the writing content, and cannot be mapped to actionable writing feedback.

7.18190351697

====================
The consistent results between all three comparisons suggest the efficacy of the segment-level classifier, and our end-to-end experimental results suggest that the predictive model is effective at providing segment level feedback.

7.17363619821

====================
For instance, len FEATURE len_extracton defines the feature returned by the user-defined function len_extracton.

7.17104862131

====================
We compute a variety of similarity measures between the input document and a sample of high and low quality documents-using both the simple TF-IDF measure used in prior work [47] as well as occurrences of popular parts of speech appearing in a document (i.e top-K nouns in unhelpful documents that appear).

7.16585939695

====================
For example, the following specify the star interface for rating and the autocomplete interface for product: CREATE INTERFACE ON reviews(rating) USING "stars" FROM "interfaces.js" AND explanation_function; CREATE INTERFACE ON reviews(product_id) USING "autocomplete" FROM "interfaces.js" AND explanation_function;  It addition, custom interfaces can be used to provide feedback that goes beyond textual feedback (e.g., visualizing distributions of common numerical values), or that is at a finer granularity than for the entire attribute.

7.15253909003

====================
[31] A. Ghose and P. G. Ipeirotis.

7.15229952917

====================
Rn be a data point (text document or segment) represented as a feature vector, where di corresponds to the value of fi .

7.15054774429

====================
The problem is exponential and we present an efficient solution that leverages the structure of random forest models to generate high-quality feedback in interactive time.

7.13521700231

====================
We showed this to be sufficient by testing on crowd-sourced labels; however more sophisticated techniques to classify segments could improve feedback.

7.12881534638

====================
D and its vote vij is the majority label in Dij .

7.12509887362

====================
Similarly, auto-complete may be used to provide feedback about existing categories in order to avoid duplicates when collecting categorical text [28, 71] (e.g., ice cream flavors, presidents).

7.10618703748

====================
Finally, when the user hovers over a highlighted segment, more targeted feedback helps explain why it was identified as low quality and how it could be improved.

7.10111867445

====================
def numeric_exp(att, val, err): return "%s: '%s' should be a number" % (att, val) CREATE EXPLANATION ON reviews(rating) FOR reviews_rating_domain USING numeric_exp; CREATE EXPLANATION ON users(age) FOR users_age_domain USING numeric_exp;  Similar functions can easily be written for the foreign-key and uniqueness constraints in Figure 4: def product_exp(att, val, err): return "%s is not a product" % val def unique_exp(att, val, err): return "%s has been taken" % val  For text attributes, the explanation function is slightly different, which is defined on a FEATURE table.

7.09460315464

====================
An example will be shown in Section 5.2.

7.09448006031

====================
def offTopic(topics, text="", feats=[]): if len(topics) < 5: sortedTopics = sorted(topics, key=topic.prob) return "Try discussing some of these topics: " + topK(sortedTopics, 5)  We used a similar process for host profiles and found that research emphasizes trustworthiness as the key quality metric [57, 94].

7.09244080406

====================
Finally, Precog explains why segments were predicted as low quality by selecting the feedback that is most relevant to changing the segment into a high quality prediction.

7.08191758568

====================
Although they are interpretable for simple constraints such as domain violations, the language for the uniqueness violation requires database familiarity and may not be accessible to non-technical experts.

7.0761837553

====================
In the rest of this paper, we use the term document to refer to the value of the acquired text attribute.

7.07087853761

====================
Our technical contribution is a pre-hoc feedback system for multi-paragraph text.

7.06984855477

====================
Our model performs competitively with prior work [32].

7.06770836916

====================
Unfortunately, this procedure is not effective for non-continuous or low cardinality features such as one-hot encoded features (e.g., each word is represented as a separate binary feature) common in text analysis.

7.06709533215

====================
To address this issue, we present a Segment-Predict-Explain pattern that  Architecture: Figure 5 depicts the system architecture.

7.0658897535

====================
In NAACL, 2015.

7.06449607399

====================
Host-Profile Participants: For the profile description experiment, we recruited 92 workers on Amazon's Mechanical Turk (58.7% male, 41.3% female, ages 20-62 uage =33, sage =8.2); all completed the task.

7.05503576132

====================
Precog is easily extended to new domains, and increases the number of high-quality documents by >= 2x compared to not using pre-hoc techniques.

7.05097141351

====================
]P(d),pi 6=0  Putting this together, we can define the responsibility score Sed of a feature explanation function (FEF) e as the average of its bound features; where Fei [?]

7.03119376813

====================
The document-level feedback is shown to the user, and the low-quality segments are highlighted as light red in the interface.

7.03109531515

====================
Specifically, we ran a crowdsourced study to label 500 Amazon segments (250 drawn from helpful reviews, and 250 from unhelpful reviews), with human helpfulness labels (the median segment length of a review is 3).

7.02813410924

====================
In contrast, Precog supports feature combinations and can dynamically generate feedback based on the  Responsibility: Our goal is to identify feature subsets of the test data point d that, if perturbed, will most improve 7  d's utility7 .

7.01897260564

====================
In this section, we describe our approach towards in-depth semantic feedback.

7.01874027254

====================
Furthermore, in some settings where collecting more data is not an option (e.g., less popular products may not have enough users that are willing to, or equipped to, write reviews), it will be more important to apply pre-hoc quality control.

7.01711029083

====================
We choose a random forest classifier, which has been shown effective in existing work [32], and select features using the recursive feature elimination algorithm [38].

7.01652849359

====================
One solution is 5 6  EXPLAIN  5.1  Problem Background  Our problem is closely related to model explanation, which generates explanations for a model's (mis-)prediction.

7.01590440633

====================
For this reason, we first define the maximum influence perturbation set PF of a given subset of features F [?]

7.00390739386

====================
Developers commonly implement explanation functions to generate more user-friendly feedback (middle column).

6.99849902852

====================
To understand the contributing factors towards the quality improvements, we compared four feedback systems that varied along two dimensions: granularity varies the feedback to be at the document level (Doc), or at the document and segment level (Seg); explanation selection compares the single-feature outlier technique from [50] (Krause) with TCruise.

6.99142631701

====================
It can be integrated seamlessly into existing crowdsourcing applications or systems with post-hoc quality control, helping them to further improve quality.

6.98410249168

====================
Upon pressing the I'm Done Writing button, the interface displayed our document-level feedback under the text field; for users in the segmentation condition, low quality segments were highlighted red and the related feedback displayed when users hovered over the segment.

6.98356480516

====================
Finally, the most sophisticated may change the input element itself in order to constrain or fully customize the feedback (right column).

6.98056450592

====================
For the sake of exposition, product_id is the textual name of the product.

6.9805116238

====================
If the features topics, featureCnt, and textLen have high responsibility, then it will be called to recommend new product features that the worker should mention in the review; the recommendations are dynamically selected based on the text's topic distribution (topics) and the number of product features detected (featureCnt < 10):  Setup: Let d [?]

6.97965976276

====================
Figure 4: Examples of three levels of Precog quality control for four classes of data integrity constraints.

6.97188014652

====================
Nevertheless, more studies are needed to fully evaluate this hypothesis across other text domains and document lengths.

6.96845568061

====================
5.3  Problem Statement  Intuition: Figure 6 depicts the main intuition behind the problem and our approach.

6.96827012066

====================
For instance, we might replace the rating domain constraint with five stars similar to Yelp and other social websites.

6.9637207617

====================
We implement a variety of length measures, and use the Apriori algorithm [75] to mine jargon based on the training data inputted into Precog, and identify its distribution across the sentences of an input document.

6.96258624721

====================
4.2  Document-level Prediction  to manually label the generated segments, but this will be very costly and time-consuming.

6.95020767963

====================
The Model Generator then trains two classification models to predict the quality of a user's overall text submission as well as its constituent segments; these are cached in the Model Store.

6.94545293344

====================
Figure 2 augments this workflow with pre-hoc quality control.

6.94458679436

====================
This problem is challenging because we must analyze potentially arbitrary text content.

6.92233393272

====================
To summarize, each participant was randomly assigned to one of four conditions: Doc+Krause, Seg+Krause, Doc+TCruise and Precog (Seg+TCruise ).

6.90904422033

====================
Indirect Quality Mechanisms: Indirect methods such as community standards and guidelines [2, 5, 70] help clarify quality standards, while up-votes and ratings provide social incentives [11, 66].

6.9077219134

====================
1 Similar to precogs in Minority Report [22], who identify and help "resolve" low-quality human action in the future, Precog identifies and helps resolve low-quality data before it is submitted in the future.

6.90050840628

====================
Quality scores are intended for attribute values for which the definition of quality defined as a continuous measure to be improved, rather than a boolean constraint, and provides the framework for which we implement a model-based feedback system for performing Precog on text attributes (Section 3).

6.89154698296

====================
* We define the Prescriptive Explanation Problem to provide actionable feedback for text acquisition.

6.88681714809

====================
Finally, we asked coders to subjectively rate their agreement from 1-7 to the statement "The post-feedback revisions improved on the pre-feedback document.

6.88159265141

====================
We directly address this problem by selecting multi-feature explanation functions to prescribe improvements to the user's text.

6.88055841975

====================
We tested this hypothesis by running an experiment, using an existing corpus of Amazon reviews [61].

6.87436029049

====================
Let minp(d, qij ) return the minimum perturbation p (based on its L2 norm) such that d matches path qij .

6.86958967545

====================
[30] H. Garcia-Molina, M. Joglekar, A. Marcus, A. G. Parameswaran, and V. Verroios.

6.86921122041

====================
We first present the results of the fully featured Precog condition (Section 7.2.1) and then demonstrate the contribution of each Precog component (in Section 7.2.2).

6.86863074144

====================
In general, we must account for the amount that a feature must be perturbed, and the number of other features that must also be perturbed, in order to improve the classification.

6.86677084847

====================
This means that for n features there are 2n possible sets of (maximal influence) perturbations to naively explore.

6.8402608891

====================
They pre-compute the "typical" values of each feature in the high quality corpus, then identify the "atypical" outliers in the test data's feature vector (e.g., a feature whose value is 1.5 standard deviations from the mean).

6.78478203341

====================
We observe that document quality is sufficiently correlated with segment quality, and a document's label can be used to label its segments as training data for a segment classifier.

6.77460934515

====================
We compared a segment binary classifier trained under this assumption with human evaluation.

6.77217918534

====================
Finally, the Similarity category reflects how many quality prediction approaches compare the input document to a gold-standard of text [47, 50].

6.76583801645

====================
Consider a single tree in a random forest, consisting of decisions on two features, len and emotion.

6.74887170215

====================
We assume that the interface is a javascript function (say, as an AngularJS [19] or ReactJS [26] component); the constructor takes as input a Precog-provided getFeedback method that retrieves feedback from the Precog server.

6.74259077261

====================
The rest of this subsection describes the DDL statements that users can use to specify feedback and interfaces for Precog quality control.

6.7254464306

====================
The first states that user information is collected from the crowd (of Amazon users) and that the username must be unique.

6.72404854369

====================
Quality Control in Crowdsourcing: Quality control is an important research topic in crowdsourced data management [16, 30, 54].

6.71989861044

====================
Thus, we decompose the text into segments, and for each low-quality segment predicted by the model, we generate segment-specific feedback.

6.69175333321

====================
Each measure is rated on a scale from 1 (Strongly Disagree) to 7 (Strongly Agree) based on coder agreement with a set of statements mapped to each criterion (i.e., "This person will stick to his/her word, and be there when I arrive instead of standing me up" for integrity).

6.68714887197

====================
Survey Design and Optimization: The survey design literature has studied ways to re-ordering, and designing survey forms in order to reduce data entry errors.

6.68315028469

====================
It uses a sliding window to compute the LDA [8] topic distribution within each window and create a new segment when the distribution changes beyond a threshold.

6.6749998619

====================
This groundwork reduces the task of applying Precog to new domains.

6.67048515418

====================
Figure 9 compares Precog against the baseline of not using Precog (naive review collection).

6.66659737487

====================
In SIGMOD, 2011.

6.66565719641

====================
TopicTiling outperformed other topic segmenters [44, 65] in terms of their WindowDiff score [74] as compared to a hand-segmented test corpus of 40 documents.

6.66423864563

====================
[34] C. H. E. Gilbert.

6.66272238542

====================
We learn this quality measure by training a random forest model that predicts the quality of individual text segments.

6.66009140235

====================
For this, we next introduce DDL statements to specify custom interfaces.

6.65325015225

====================
The developer then binds an explanation fuction to the appropriate constraint.

6.6504212646

====================
We describe this in Section 4.

6.6467790566

====================
For instance, the bottom row of Figure 4 illustrates fine-grained feedback in the form of both highlighted text and text feedback for individual segments that the user has written for reviews.review.

6.64660121625

====================
First, the quality measure is continuous (there is no "perfect document") and thus hard to identify a "violation".

6.64218779636

====================
The classic approach is to use simple, interpretable models [13, 53, 88] or to learn an interpretable model using the training  https://www.reddit.com/r/askscience/ We define > 1 net up-votes as helpful and <= 1 as unhelpful.

6.63937903951

====================
71.3% had written a prior product review; all had read a product review in the past.

6.63843573268

====================
One approach is to simply highlight the low-quality segment and provide generic/static feedback.

6.61694303684

====================
User-facing Interface: The custom interface column for the quality score in Figure 4 depicts the Precog interface in action.

6.61684784294

====================
Based on this library, we develop document-level and segment-level prediction models.

6.61657667716

====================
We created a simple Mechanical Turk task that asked workers to submit the product model of their cell phone along with a 1 to 5 rating for the phone's quality; each 9  worker was paid $0.05 to complete the task.

6.60720693691

====================
Although the data cleaning literature has proposed ways to prescribe data cleaning operations [14], they are not applicable for text attributes.

6.60097150727

====================
The following snippet sketches the Not Enough Detail function in our evaluation.

6.6001809006

====================
All trustworthiness factors except friendliness directly corresponded to existing explanation functions.

6.58722449397

====================
To do so, we first define the impact I(d, p) for an individual perturbation p as the amount that it improves the utility function discounted by the amount of the perturbation [?

6.58561485085

====================
It does so by generating feedback or interface changes to help workers improve their data pre-submission.

6.57899207112

====================
We defer this to future work.

6.57577357418

====================
Once a library of features are given, the document-level prediction turns to be a typical classification problem.

6.56747313384

====================
Our approach is inspired by existing feedback systems--model features act as signals to identify text characteristics that the worker should change.

6.56282151793

====================
The Journal of Forensic Psychiatry & Psychology, pages 1-21, 2017.

6.56047158555

====================
However, it leaves it up to the user to infer specific improvements to make.

6.55213306973

====================
This results in a 2x2 between-subjects design.

6.54892000126

====================
For both Product Review and Host Profiles, we performed Two-Way ANOVAs with both the Overall Quality Im11  knowledge, Precog is the first system that systematically supports Precog for a wide range of data types and quality specifications (constraints and quality scores).

6.54247678909

====================
Developers can bind the interface to an attribute using a CREATE INTERFACE statement.

6.54118566067

====================
Rmxn represent the features bound to each of the m FEFs, where Aji = 1 if feature fi is bound to FEF ej , ~ e otherwise 0.

6.53771737857

====================
We thus assign each participant to one of four conditions.

6.53373942852

====================
The green path (p1 ) must at least reduce emotion by -20; the blue path (p2 ) must at least increase length by 10 and at least reduce emotion by -15.  input text.

6.53265614641

====================
In order to generate targeted feedback, Precog automatically identifies topically coherent portions and segments the document in order to analyze each segment individually.

6.53055811616

====================
I(d, p) =  We instead present a heuristic solution called TCruise whose complexity is linear in the number of paths in the random forest model.

6.5247746378

====================
While such approaches are often supervised in nature, requiring a manual topic ontology [57, 62], we use LDA [8] because it is unsupervised and can be quickly trained on any corpus without any cost to the developer.

6.51491115058

====================
We found that combining segmentation and TCruise-based explanation outperformed all other conditions by a statistically significant margin for Product Reviews, and outperformed all but the next-best Doc+TCruise condition for Host Profiles.

6.5141438995

====================
Fminp(d,q) = Fminp(d,q0 ) }  Problem 1 (Prescriptive Explanation).

6.50956303688

====================
Figure 10 plots the mean change and 95% boostrap confidence interval for the four rubric scores.

6.50798819084

====================
The Segment-PredictExplain component has a beige background: Blue arrows depict the offline training and storage process and Green arrows depict the online execution flow when a user submits.

6.50790155495

====================
In ACM SIGKDD workshop on human computation, 2010.

6.50085060999

====================
The feedback literature suggests that precise, local feedback is effective [68].

6.49908267389

====================
The third coder was trained by being shown the Amazon or Airbnb corpus, examples across the quality spectrum, and the other two coders.

6.49688663681

====================
In summary, we find that TCruise is essential to improving document quality; combining TCruise with Segmentation empirically produces the best results across the board.

6.49131620272

====================
In this example, there are two ways to perturb the feature vector: by reducing the emotion feature by at least 20, or by increasing the length by at least 10 and reducing the emotion by at least 15.

6.48657177732

====================
Custom Interface: Fully customizing the interface component is useful in order to directly prevent users from submitting invalid attribute values.

6.48620432194

====================
For instance, in a binary classification problem U may return 1 if the input is "high quality" and 0 otherwise; in a regression model, U may be the identity function.

6.48235970702

====================
By tackling low-quality data pre-acquisition, it can reduce or eliminate the need for post-hoc quality control.

6.47567404303

====================
The online components (green arrows) send the contents of a text input widget, along with an optional corpus name, to the webserver.

6.45606325268

====================
Figure 11 shows a similar chart for the coder's subjective opinion of the improvement.

6.45332859616

====================
Precog uses existing techniques to generate forms for crowd workers to fill out, and the form contents are inserted as new records into the corresponding crowd table.

6.44967321573

====================
Feature Library for Text Quality  PREDICT  Precog takes as input a training corpus of documents and document-level quality labels, and trains two models-- document-level and segment-level prediction models--in order to provide document-level and fine-grained segment-level feedback.

6.44705932126

====================
Second, we evaluate Precog's Segment-Predict-Explain pattern for text acquisition in two domains--acquiring customer reviews for Amazon products [61] and acquiring profile descriptions for AirBnB host profiles [94].

6.44700292829

====================
These plots show the effect size across all measures, and that the largest improvements were due to the combination of segmentation and TCruise-based explanation.

6.44385869784

====================
Feedback systems are typically based on outlier detection [50].

6.44340165429

====================
For instance, F may be the text features described above, and a data point corresponds to the extracted text feature vector.

6.43918812483

====================
The second states that a review is written for a given product in the products table, and contains a numerical rating as well as the text of the review.

6.43711414696

====================
We are optimistic about the Segment-Predict-Explain pattern, because adopting to new domains is simply a matter synthesizing existing research by adding features and creating simple explanation functions.

6.43232952249

====================
Social computing and user-generated content: a game-theoretic approach.

6.42948254748

====================
]Q (d) i i  The space of solutions for Problem 1 relies on enumerating all possible elements in the power set of the feature set F, which is exponential in size: 2|F | .

6.42944701409

====================
The user writes a product review in the textbox; the content is sent to the Precog backend via getFeedback().

6.42627233655

====================
In these examples, we simply define a python function.

6.42061011552

====================
Each facet defines high quality at a different threshold; product reviews and host profiles are shown as the top and bottom rows, respectively.

6.4154095643

====================
We describe our process to extend Precog to two domains with different quality measures: product reviews that care about helpfulness to a shopper [3], and then host profiles that are judged by trustworthiness to renters [57].

6.41082592731

====================
Procedures: Participants writing product reviews were asked to write a review of their most recently owned laptop computer "as if they are trying to help someone else decide to buy that laptop or not and are writing on a review website like the Amazon store".

6.40274945655

====================
[26] A. Fedosejev.

6.39836994372

====================
Furthermore, controlling for the other variable, TCruise showed a statistically significant difference in improvement, while segmentation did not.

6.39778597897

====================
[29] J. Gao, Q. Li, B. Zhao, W. Fan, and J. Han.

6.3941547566

====================
Segment-Predict-Explain: Based on these observations, Precog automatically identifies low-quality portions of a document, and generates feedback to help improve the identified issues.

6.39386068335

====================
Existing feedback approaches are not directly applicable for Precog.

6.39336549044

====================
Given a small test corpus of pre-segmented documents, Precog can benchmark the algorithms and recommend the one with the highest WindowDiff score.

6.39160226784

====================
](p) and the model's prediction confidence C(d + p) [?]

6.38954353922

====================
The default simply renders feedback generated from database constraint violations on tuple insertion (left column).

6.38194296079

====================
Note that the same explanation 2  Note that the developer may express a CHECK constraint and the database can generate an (indecipherable) error message.

6.37640625921

====================
62% had used AirBnb before.

6.37370320351

====================
* The design and implementation of Precog, which supports pre-hoc quality control for constraints over simple data types and quality measures over text and open-ended attributes.

6.36935029241

====================
At this point, users could click the Recompute Text Feedback button (median 1 click/participant), or press Submit to submit and finish the task.

6.36832275765

====================
Feedback and interface customization acquire 1.7x and 1.9x more valid records than no Precog optimization.

6.36346678787

====================
* A data-driven approach to estimate quality for text attributes, including a categorization and implementation of 47 text quality features from a survey of the literature.

6.36148761326

====================
Customized Feedback: Precog provides a DDL for developers to customize feedback.

6.36098416355

====================
F |pi 6= 0}  Finally, Sfdi computes the responsibility for fi as the sum of all maximal influence paths in all decision trees that improve the predicted utility U ().

6.36053157607

====================
When the topic within the window changes significantly, then TopicTiling creates a new segment.

6.35741966334

====================
Through extensive MTurk experiments, we find that Precog collects >= 2x more high-quality documents and improves text quality by 14.3% compared to not using pre-hoc techniques.

6.35234660556

====================
8  We added LIWC API calls to Precog; the model tested on a balanced set of 300 AirBnB host profiles was competitive (79% accuracy) at predicting if a profile was >= median trustworthiness.

6.35088928069

====================
al describe the meaning of the three Airbnb criteria in [57]: Ability "refers to the host's domain specific skills or competence."

6.35047207569

====================
Consider a review consisting of a long, angry diatribe about customer service.

6.34968686222

====================
Cost Savings  Figure 10: Improvement on Likert scores for both domains (reviews and profiles) and four quality criteria per domain.

6.34665579047

====================
SEGMENT-PREDICT-EXPLAIN  The challenge with directly developing Precog quality control for text is that the quality score and explanation function is difficult to express as a concrete function, and they must be customized for the application domain.

6.34621971574

====================
In contrast to typical integrity constraints, which will reject an inserted record that violates the constraint, Precog seeks to maximize its value.

6.34560885995

====================
reduces the developer's efforts by allowing them to express the quality score in terms of model features by defining a FEATURE table, and to define explanation functions over features of the text attribute.

6.34423822629

====================
, Tt } is composed of a set of trees.

6.34318472947

====================
3  function is used for domain constraints on reviews.rating and users.age.

6.34309518221

====================
However, we may use a slider if for larger cardinalities.

6.34165294943

====================
The offline components (blue arrows) take as input a corpus of training data in the form of user generated text documents and their labels--for instance, Amazon product reviews may be labeled by the ratio of "helpful" and "unhelpful" votes.

6.33990312342

====================
Further, their analyses are per-feature and don't account for multi-feature interactions.

6.33774473083

====================
Since the quality score is not a boolean constraint, feedback is simply not generated for it2 .

6.33766642072

====================
This can be directly computed by examining the decision points along the path.

6.3352650253

====================
The Feedback Generator then constructs feedback explanations for the low quality text, which are returned and displayed in the widget.

6.33271266359

====================
7.2  Precog for Text Acquisition  Setup and Datasets: Precog is setup as described in Section 6: we train Precog using the laptop category of the Amazon product reviews corpus [61], and the AirBnb profile corpus [94] for their corresponding experiments.

6.33207792652

====================
The Segmenter first splits each document into segments.

6.32910254774

====================
Overall, each explanation function was 3-20 lines of python code.

6.32642682967

====================
4.1  Existing automated writing feedback tools primarily focus on syntactic, simple errors [27, 35, 63].

6.32500008766

====================
Additionally, our Segment-Predict-Explain pattern addresses on free-form text entry that complements their focus on simple data types.

6.32184242074

====================
The document level feedback provides a global quality assessment.

6.32095507338

====================
Specifically, we develop effective approaches to measure text quality at both document and segment levels, present an efficient technique to solve the prescriptive explanation problem, and discuss how to extend Precog to new domains.

6.31760165859

====================
As constraints become more complex, there is a need for customized messages.

6.31265709195

====================
[36] R. M. Groves, F. J. Fowler Jr, M. P. Couper, J. M. Lepkowski, E. Singer, and R. Tourangeau.

6.31109079977

====================
2  2.

6.31074736788

====================
We did not require new features for product reviews; we simply label reviews with >= 60% helpful votes as high quality and low otherwise.

6.30539692924

====================
When we consider a user's edits, they are desirable if the edits will improve the document's quality--in other words, if it will cause the document to be reclassified as high quality.

6.3032467589

====================
Some constraints, such as domain constraints, are registered as syntax errors.

6.30065882488

====================
Finally, we perform a detailed analysis of how segmentation and TCruise each contribute to improving the quality of the acquired text.

6.29958187806

====================
3 Note that databases automatically generate names for almost all integrity constraints.

6.29262075686

====================
Precog is agnostic to the specific segmentation algorithm, and developers can use their own.

6.28973217964

====================
Thus, we wrote a friendliness explanation function that suggested writing more friendly and inclusive prose, and bound it to the relevant LIWC features (social, inclusive, etc).

6.28850665723

====================
In addition to boolean constraints such as domain, foreign key, and uniqueness constraints, Precog also supports quality scores.

6.28607954732

====================
In many cases, such as text attributes, it is desirable to provide feedback for specific segments of the text value.

6.2854385234

====================
The key insight is that the predictive model is robust to noisy labels.

6.28464408832

====================
Moreover, there have been many successful attempts to use topic distributions to predict quality [55, 57, 57].

6.28312041454

====================
For instance, qualreview seeks to maximize the quality score as defined by qual_udf.

6.28277275768

====================
In Eighth International AAAI Conference on Weblogs and Social Media, 2014.

6.28109704322

====================
The general approach is to survey quality assessment research in a domain to borrow useful features and explanations.

6.280536355

====================
Although these user defined functions are powerful enough to support arbitrary analysis of an attribute value, such an approach is difficult to compose and extend, and the feedback is still limited to the entire attribute value.

6.27786632864

====================
Assuming that C() = 1, p1 's impact on the input document is I(d, p1 ) = 1-0 x1= 20 0.05, whereas p2 's impact is I(d, p2 ) = 1021-0 x 1 = 0.055.

6.27027394122

====================
For these, Precog generates default names of the form <table>_<attribute>_<type>.

6.26925258161

====================
Document Labels for Segments: Despite generating topically coherent segments, we lack quality labels for training the predictive model at the segment level.

6.25707403123

====================
Figure 7: Worker interfaces to evaluate no optimization, custom feedback Precog, and custom interface Precog for hard constraints.

6.25227629676

====================
In contrast, segment level feedback is needed in order to provide specific, actionable suggestions that may not be evident at the document level.

6.24718670972

====================
First, we validate the value of pre-hoc quality control by running a crowdsourced data acquisition experiment with different Precog optimizations for foreign key and domain constraints.

6.24403643874

====================
database, which checks that the input satisfies the integrity constraints.

6.23945802497

====================
The resulting model (85% accuracy, balanced test set) was competitive with existing work [31].

6.23709556075

====================
Both are important because they address different text quality factors.

6.22914087179

====================
Section 3 describes the Segment-Predict-Explain pattern that helps developers easily customize interface for text attributes.

6.2270538503

====================
Although there are numerous segmentation algorithms, we describe the rationale for the choice of using a topic-based segmentation algorithm.

6.22550165237

====================
For instance, consider a document that contains a single segment--the segment may be high quality but the overall document is too short and is missing text for other topics.

6.22393966416

====================
The Segment-Predict-Explain component consists of offline and online components.

6.22377796749

====================
The key challenge is that training data only contains quality labels for entire documents (e.g., helpfulness for the full review), and it is unclear how to leverage them for training a segment-level model.

6.21721652808

====================
3.

6.21596018126

====================
Further, developers can easily extend the library with custom features.

6.21231978207

====================
These naturally map to 4 of our feature categories, so we wrote explanation functions for each and bound them to the features in the corresponding category.

6.20852780207

====================
Their work identified a subset of the Linguistic Inquiry and Word Count (LIWC) features [73] and other features as useful for measuring trustworthiness.

6.20576921179

====================
6  data near the test point [76].

6.20546495415

====================
We evaluate Precog for product_id (foreign key constraint) and rating (domain constraint) from the reviews table.

6.20349968061

====================
On violations, the feedback generator creates custom feedback (if specified in a DDL statement) and the default or customized interface displays the feedback.

6.19881096387

====================
We identify five main categories across the existing literature (Table 1).

6.19714148232

====================
However, such systems would not recognize that the review can be most improved by simultaneously reducing the emotion in the text and including more product details that ultimately increase the length.

6.19687723484

====================
We now describe the prediction model we use for documentlevel prediction.

6.19680097132

====================
To ensure fair comparison, we supplemented their features with domain-specific features for Informativeness (# of product features/jargon), Readability (Coleman-Liau index), and Friendliness (LIWC features related to friendliness) so that their features are comparable to those used in our feature library.

6.19226691041

====================
A closely related work from the database community is Usher [15], which have similar goals to improve data collection quality.

6.19037426448

====================
Clearly, document quality assessment is a well-studied area.

6.18921456894

====================
A similar approach is applicable for regression models as well, where increasing the continuous prediction assigns the perturbation more responsibility.

6.1822284754

====================
Figure 5: Precog architecture.

6.17990257226

====================
Precog uses the models in the Model Store to identify whether the entire document and/or segments generated by the Segmenter are low quality.

6.17897794686

====================
Precog uses the feature library to transform the input text into a feature vector of (len=10, emotion=30), and is thus classified as low quality.

6.17531371392

====================
4.3  Segment-level Prediction  There are two challenges in training a segment-level prediction model.

6.1737510376

====================
The final submission was considered the post-feedback submission, and the initial submission upon pressing the I'm Done Writing was the pre-feedback submission.

6.17299630926

====================
Thus, Precog segments documents at topic-level units.

6.1651172084

====================
We trained workers on a separate sample of segments, along with explanations of why each segment was helpful or unhelpful.

6.16252777619

====================
We address these challenges by proposing a novel segment-predict-explain pattern for detecting lowquality text and generating prescriptive explanations to help the user improve their text.

6.16044694814

====================
[39] D. Haas, J. Ansel, L. Gu, and A. Marcus.

6.15862911365

====================
4.

6.15679200609

====================
Features are individually mapped to pre-written feedback text [7, 10, 49].

6.14721664061

====================
Figure 8 plots the number of high quality tuples that were collected as a function of the number of completed tasks; we define a tuple as high quality if no constraints were violated.

6.14625699329

====================
](p) = |p|2 , the L2 norm of the perturbation vector.

6.14615274321

====================
Precog denotes the segment-level TCruise-based system.

6.14355188159

====================
Ultimately, existing feedback and explanation approaches are descriptive of the prediction, rather than prescriptive of the changes that must be made.

6.14255628911

====================
Moreover, Precog also makes it easy for developers to add custom segmentation algorithms.

6.14054623596

====================
Estimating the helpfulness and economic impact of product reviews: Mining text and reviewer characteristics.

6.13662490495

====================
Further, the set of maximum influence perturbations is the union of PF for all feature sets:  minp(d, qij ) = arg min |p|2 s.t.

6.13515351467

====================
Given the feature vector of a data point d, prediction model M , a set of FEFs E = {e1 , * * * , em }, return the top k FEFs whose responsibility is above a threshold t:  Fp = {fi [?]

6.13008902348

====================
[41] J. Hyona, R. F. Lorch Jr, and J. K. Kaakinen.

6.12223083501

====================
[6] M. S. Bernstein, G. Little, R. C. Miller, B. Hartmann, M. S. Ackerman, D. R. Karger, D. Crowell, and K. Panovich.

6.12072163861

====================
The second challenge is to determine how the available document-level labels can be used for training segment-level quality.

6.11865812783

====================
The first one is how to split a document into segments.

6.11543401497

====================
An FEF e : R|F| - text maps the feature vector for a subset of features F [?]

6.11387763583

====================
The main idea is to scan each tree in the random forest and compute responsibility scores local to the tree.

6.11273460849

====================
We now formally define these feature-oriented explanation functions (FEFs) and provide examples used in the experiments.

6.1107473998

====================
81 completed the task.

6.10827234929

====================
Feature Explanation Functions  Section 2 introduced explanation functions that can take as input features in a FEATURE table whose primary key references the desired text attribute.

6.10756546983

====================
[9] R. Boim, O. Greenshpan, T. Milo, S. Novgorodov, N. Polyzotis, and W. C. Tan.

6.1057208115

====================
Dij |yk = vij }| |Dij |  Qi (d) = {q [?]

6.10453340117

====================
, dm } be the training dataset and Y = {y1 , .

6.10259755813

====================
We assume that the domains of the features have been normalized between [0, 1].

6.10142196011

====================
We describe how Precog automatically generates feedback text for low-quality text.

6.10134407841

====================
Thus, it is clear that the emotion should be assigned a greater responsibility because there are more branches for which changing its value will contribute to a better classification.

6.09539979115

====================
5.

6.09275875881

====================
The average task completion time was 14 minutes, and payment was $2.5 (~ $10/hr).

6.0895878301

====================
Precog can automatically control the generated feedback by reallocating responsibility.

6.08726152463

====================
However, it still leaves it up to the user to infer specific improvements to make.

6.08542775765

====================
A path is the sequence of decisions from the root of a tree to a leaf node.

6.0844568641

====================
Let F be the set of n model features, and fi denote the ith feature.

6.07971734621

====================
Pn j Aji is the average i=1 impact score of all features mapped to the jth FEF.

6.07714079663

====================
Rn is a vector that modifies a data point.

6.0720041806

====================
pi 6= 0 if fi is perturbed, otherwise pi = 0.

6.07008196709

====================
Dij ), and the output of the random forest M (d) = arg maxv |{1|vij = v}| is the majority vote of its trees.

6.06856635341

====================
Subjectivity is the extent that the review is fair and balanced but with enough helpful opinions for the buyer to make an informed decision: 1 means the review is an angry rant or lacks any opinions while 7 means it is a fair and balanced opinion.

6.06625886237

====================
Precog is able to adopt to the domains' different quality measures (helpfulness vs trustworthiness) with small configuration changes.

6.06600973993

====================
A model M : Rn - N classifies a data point as M (d) [?]

6.06540544727

====================
TKDE, 2016.

6.06477805769

====================
PVLDB, 2015.

6.06463147901

====================
In future work, we hope to explore a broader range of applications (e.g., different social media domains or user contexts), and study how to optimize data-collection interfaces to meet more complex application needs.

6.06392692814

====================
The full set of coder statements is described at length in [57].

6.06028076008

====================
Intuitively, the FEF should be executed if its list of features F can take "highly responsibility" for improving the quality score.

6.05933997507

====================
Truth discovery and crowdsourcing aggregation: A unified perspective.

6.05377599091

====================
Recall the feedback in the custom Precog interface in Figure 4, it identifies that the segment is short on details and suggests new topics.

6.04756701896

====================
]q0 [?

6.04684760377

====================
Though Precog demonstrates the feasibility of such automated interfaces, it also reveals several areas of improvement.

6.04682941964

====================
In practice, an FEF takes as input a list of features, as well as the text document and the full feature vector, and returns feedback text.

6.03893415257

====================
F to feedback text.

6.03861083484

====================
We conduct statistical tests to further investigate the results.

6.034658945

====================
, ym } be their labels.

6.03387676926

====================
A tree Ti is composed of a set of k decision paths qi1 , .

6.02838075147

====================
In addition, rather than compute the impact for all possible perturbations, we only consider the minimal perturbation with respect to each path in the tree.

6.02670523863

====================
Incentive mechanisms such as badges, scores [21, 33], status [97], or even money [42, 46] have also been used to keep good contributors.

6.02620973288

====================
Normalization: We find that features closer to the root will happen to occur in more feature sets and have artificially higher scores, thus we need to adjust feature impact scores to reduce bias.

6.02599023576

====================
N, and a utility function U : N - R maps a label to a utility value.

6.02576575173

====================
Each defines the three main measures, and provides examples that contribute positively and negatively to each criteria.

6.0220524601

====================
Let D = {d1 , .

6.02101625913

====================
i  Picking FEFs: Once the feature scores have been computed, identifying the top-k FEFs is straightforward, and we compute each FEF's average impact score using a series of fast matrix operations.

6.01950164535

====================
A perturbation p [?]

6.01833528557

====================
However, the combination of segmentation and TCruise consistently produced larger effect sizes than all other conditions across both Host Profiles and Product Reviews: for Product Reviews Precog, which combines segmentation and TCruise, improved the overall measure (bottom left facet) by nearly 3.9x over the baseline (0.55 vs. 0.14 increase), and a 2.4x improvement over the next-best Doc+TCruise condition.

6.01540110668

====================
]Fe  C(d) =  i  |Fe |  x C(d + minp(d, qij ))  |{dk [?]

6.01468694559

====================
The confidence C(d) is the fraction of samples in Dij whose labels yk match the path's prediction vij .

6.0133316119

====================
2 +15 However, there can be an infinite number of perturbations that all improve the utility--which should be selected?

6.01048872562

====================
The key insight is to take advantage of the structure of the random forest model to constrain the types of perturbations and feature subsets to consider.

6.00653199215

====================
The random forest model M = {T1 , .

6.0022822863

====================
[0, 1].

5.99991836851

====================
](p)  C can be chosen based on the model--for a random forest, we define C as the percentage of trees that vote for the majority label.

5.99918815124

====================
For instance, the following defines the function for Off-Topic text:  7.1  Precog for Hard Constraints  Although it is intuitively obvious that form feedback and custom interfaces should improve quality, we quantify the amount using the example from Section 2.

5.99874847432

====================
Further, review hierarchies were proposed for hierarchical crowdsourced quality control using expert crowds [39].

5.99787273305

====================
Thus, the output of Ti (d) is the vote vij of the path that matches d (e.g., d [?]

5.99555313891

====================
Overall Quality is the holistic helpfulness of the review for prospective buyers.

5.9949286149

====================
The impact function I() is identical, however it takes a path qij as input and internally computes the minimum perturbation minp(d, qij ).

5.99427341589

====================
It will cause the impact function to converge to 0 as the perturbations become larger.

5.98407375345

====================
10  7.2.2  Ma et.

5.97721499276

====================
For product reviews, Informativity is the extent that the review provides detailed information about the product, where 7 means that the review elaborates on all or almost all of the specifications of a product while 1 means that it states an opinion but fails to provide factual details (e.g., laptop specifications).

5.97049955448

====================
Participants were told that upon submitting their writing, they may receive feedback and could optionally revise.

5.97041960186

====================
[4] Y. Attali and J. Burstein.

5.97033806899

====================
]E  Sfdi =  The TCruise Heuristic Solution  X  X  I(d, qij ) if U (vij ) > U (M (d))  Ti [?

5.9687224414

====================
Crowddb: answering queries with crowdsourcing.

5.96521231206

====================
These include guidelines and constraints on form elements [36, 69], as well as interface techniques such as double entry [20] commonly used for picking passwords.

5.96314938789

====================
U (M (d + p)) - U (M (d)) x C(d + p) [?

5.96158105827

====================
[23] M. Drouin, R. L. Boyd, J. T. Hancock, and A. James.

5.96057450109

====================
For instance, a review that consists of many ambiguous phrases like "I have never done anything crazy with it and it still works."

5.95935237428

====================
can be similarly defined in multiple ways.

5.95835646784

====================
.

5.9578460418

====================
Post-hoc Approaches for Text Acquisition: A dominant approach is to filter poor content [84] such as spam; sort and surface higher quality content [1, 37, 86] such as product reviews [67], answers to user comments [90, 95], or forum comments [82]; or edit user reviews for clarification or grammatical purposes [6, 42, 48].

5.95542787402

====================
The discount function [?]

5.95477833782

====================
For instance, consider [?

5.95422807848

====================
F is based on the responsibility of each perturbation that involves the feature.

5.95285828623

====================
Rn where ~si = Snormdi , and matrix A [?]

5.95085597688

====================
Consider the perturbations p1 , p2 in Figure 6.

5.94989531321

====================
E * = topk Sed s.t.

5.94555053534

====================
F as the set of perturbations that only perturbe features in F and have the maximal influence.

5.94428720006

====================
Precog only marginally increases latency of each worker.

5.94130829784

====================
We used a post-study survey to collect demographic information as well as their subjective experience.

5.93851106009

====================
We now describe related work in terms of data acquisition interface optimizations, quality control in crowdsourcing and other post-hoc quality mechanisms specific for text acquisition.

5.93642013435

====================
qij matches d + p p[?

5.93625352716

====================
]Rn  PF (d) = arg max I(d, p) s.t.

5.9358779063

====================
]F  Based on these definitions, the total responsibility of a given feature fi [?]

5.93099783496

====================
]F / (pi = 0) p[?

5.92558794704

====================
Given d and predicted utility U (M (d)), we retrieve and scan the paths with higher utility.

5.92328017257

====================
Thus, three of the FEFs, Informativeness, Topic, and Readability/Grammar, overlapped between the two domains.

5.92193018094

====================
]fi [?

5.92016847627

====================
[?

5.91762124049

====================
PF (d)  F[?

5.91688891646

====================
F is the set of features bound to an FEF: P Sed =  fi [?

5.91207460878

====================
For each feature fi , we compute the responsibility for each low quality text, and aggregate their values to compute the sample mean ufi and standard deviation sfi .

5.91201456346

====================
Readability is the extent that the review facilitates or obfuscates the writer's meaning.

5.9109898771

====================
[43] P. G. Ipeirotis, F. Provost, and J. Wang.

5.90437474933

====================
]Ti I(d, q) >= I(d, q 0 ) [?]

5.89942920565

====================
For each scanned path q, we compute the change in the utility function, discount its value by the minimum perturbation p as well as the path's confidence.

5.89670388095

====================
Figure 9: # of documents where quality >= thresh, for varying thresholds; product reviews (top), host profiles (bottom).

5.89659191976

====================
Participants were randomly assigned to one condition group; with (21,26,22,23) participants in conditions (1,2,3,4), respectively.

5.89588570919

====================
Ti |[?

5.89377013049

====================
Also, let ~e [?]

5.89017152004

====================
We relaxed the foreign key constraint by ignoring case sensitivity of the product names.

5.88812004881

====================
Let ~s [?]

5.88804750077

====================
Sed > t e[?

5.88734690751

====================
](minp(d, qij ))  If two paths within a tree perturb the same set of features, we only consider the path with the maximal impact score.

5.88465614798

====================
Rm = A~s.

5.88218907827

====================
We define Qi as the set of maximal impact paths of a tree Ti , with at most one path for a given subset of features.

5.88107836609

====================
In addition, we do not compare paths across trees.

5.88103470598

====================
]M q j [?

5.8808976737

====================
Argonaut: macrotask crowdsourcing for complex data processing.

5.87427630269

====================
7  [?

5.87224126868

====================
For the foreign key constraint, we populated a products table with all cell phone product models from the Amazon product corpus and a comprehensive list of phone models [93].

5.87104113605

====================
impact paths for each tree; for each path, we add the responsibility score of all features perturbed in its minimum perturbation p. The final scores are used to select from the library of explanation functions.

5.86913715065

====================
Other explanation functions (Topic, Informativeness) suggested specific content for the user to write about, mined from high-quality documents from each corpus (i.e., topics, jargon).

5.86852283159

====================
Designing novel review ranking systems: predicting the usefulness and impact of reviews.

5.86101090828

====================
Usher analyzes an existing corpus of collected data to dynamically learn soft constraints on data values, and focuses on input placement, re-asking, and some interface enhancements.

5.85523621448

====================
To do so, we draw a sample of text from the corpus that has been labeled as low quality.

5.85113094618

====================
How much work does it take to add rich feedback support for text in a new domain?

5.84687864654

====================
However this work either focuses on a particular application [91], or not intended to support custom interfaces [72].

5.8450036827

====================
Figure 11: Subjective agreement to: "The post-feedback revisions improved on the pre-feedback review."

5.84472172017

====================
The experiment was run until 100 workers had participated in each condition.

5.83951975377

====================
behind-the-enemy-lines.com/2011/04/want-to-improvesales-fix-grammar-and.html, 2016.

5.83608742586

====================
The average task completion time was 11 minutes, and payment was $2.5 (~ $13.6/hr).

5.83262318558

====================
To the best of our 12  10.

5.82669595

====================
Each measure is the average of the ratings from two coders--if they differed by >= 3, a third expert coder was used as the tie breaker and decided the final value.

5.82577319059

====================
Figure 7 depicts the three interfaces that are created--naive with no Precog, customized feedback, and customized interface optimizations.

5.81966549075

====================
We start with the feature library of 47 features and no explanation functions.

5.81768079786

====================
Challenges in data crowdsourcing.

5.81737222633

====================
The primary groups of features related to absence of detail and low topic diversity.

5.81515496317

====================
6.

5.81395908306

====================
Due to a small number of explanation functions, study participants found that repeatedly using the system began to provide redundant feedback; simplifying the development of more explanation functions may help the system produce more nuanced feedback.

5.80668598884

====================
Packt Publishing Ltd, 2015.

5.80357462715

====================
The experiment was IRB approved.

5.80128495175

====================
is assigned 1 as it might require multiple readings to understand.

5.78966165765

====================
Reading through their table of features, we also found that writing style and friendliness features were common.

5.78714263139

====================
RELATED WORK  9.

5.7848737132

====================
Participants were randomly assigned to one condition group; all conditions had 21 subjects except the Precog condition which had 22.

5.77896566505

====================
Check spelling and grammar in google docs.

5.77519911166

====================
For the reviews and profiles experiments, Precog acquires >= 2x and >= 2.6x more high quality documents than the baseline for thresholds of 5.5 and 6, respectively.

5.77020816018

====================
The average host profile took 6.8 minutes to complete without Precog, and 11.1 minutes with the additional feedback from Precog.

5.76968172459

====================
For Host Profiles Precog improved the overall measure by nearly 7.1x over the baseline (0.65 vs. 0.07 increase), and a 1.7x improvement over the next-best Doc+TCruise condition.

5.76374838388

====================
Participants writing Airbnb profiles were asked to "pretend that [they] are interesting in being a host on Airbnb" and to "write an Airbnb profile for [themselves]".

5.75895422372

====================
[8] D. M. Blei, A. Y. Ng, and M. I. Jordan.

5.75822690083

====================
A method to automatically choose suggestions to improve perceived quality of peer reviews based on linguistic features.

5.75682587976

====================
Figure 8: # records satisfying both constraints vs budget.

5.75509659362

====================
[18] L. Connors, S. M. Mudambi, and D. Schuff.

5.74862566143

====================
We used a qualification task to ensure participants had ever owned a laptop.

5.74293660091

====================
No participant had used Precog before.

5.73672300174

====================
provement and Subjective Coder Improvement Scores as the dependent variables, and TCruise and segmentation as the independent variables.

5.735398794

====================
Feedback and interface customization acquire 1.7x and 1.9x more high quality tuples than no Precog optimization.

5.73327868891

====================
The institutionalization of youtube: From user-generated content to professionally generated content.

5.73110452272

====================
Similarly, Airbnb profiles took an average of 10.2 minutes to complete without Precog and 15.3 minutes to complete with Precog.

5.72336853939

====================
Such latency difference is relatively small if we compare the end-to-end time of two systems since the majority of the time was spent on worker recruitment.

5.71873884415

====================
ACM, 2012.

5.7167873625

====================
The change in these measures between pre and post-feedback suggests the utility of the feedback.

5.71588798487

====================
There are some works that apply pre-hoc quality control to improving crowd quality [72,87,91].

5.71562362672

====================
Write smarter emails.

5.70108203162

====================
We plot CDF curves for the number of high quality documents as the task budget increases.

5.70060775513

====================
[71] A. G. Parameswaran, H. Park, H. Garcia-Molina, N. Polyzotis, and J. Widom.

5.68707047584

====================
Scoring persuasive essays using opinions and their targets.

5.68625916365

====================
for product reviews, and "The post-feedback revisions are more trustworthy than the prefeedback profile."

5.6817601465

====================
The interface was the same for all conditions--only the feedback content changed.

5.67647981215

====================
Crowdforge: Crowdsourcing complex work.

5.67381521241

====================
Are both Segment and Explain necessary in the SegmentPredict-Explain pattern?

5.67199456894

====================
[54] G. Li, J. Wang, Y. Zheng, and M. J. Franklin.

5.67122380995

====================
Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission.

5.66717549689

====================
Benevolence "refers to the host's domain specific skills or competence."

5.660475605

====================
Ability "refers to the host's domain specific skills or competence."

5.65811972752

====================
Overall Quality is the holistic trustworthiness of the host for prospective tenants.

5.6575419638

====================
Crowdsourced data management: A survey.

5.65287850197

====================
In ETS Research Report Series.

5.65243418101

====================
", or 0 if the document did not change.

5.645832119

====================
The coders labeled documents in random order and did not have access to any other information about the documents.

5.6442375076

====================
Social recommender systems.

5.63856699435

====================
7.2.1  Segment, Explain, or Both?

5.6345800815

====================
[38] I. Guyon, J. Weston, S. Barnhill, and V. Vapnik.

5.63423252685

====================
Note that the quality criteria differ across domains.

5.62207157286

====================
These approaches incur additional quality control costs and are complementary to Precog.

5.62041642307

====================
It has been extensively studied in recent years [9,12,24,29,39,80,98].

5.61969167546

====================
[49] J. Krause, A. Perer, and K. Ng.

5.61963499478

====================
Texttiling: Segmenting text into multi-paragraph subtopic passages.

5.61590975168

====================
Precog is more effective than no Precog when the desired quality is high.

5.61491061701

====================
[40] M. A. Hearst.

5.61310573063

====================
[1] E. Agichtein, C. Castillo, D. Donato, A. Gionis, and G. Mishne.

5.60611800127

====================
[11] A. Bosu, C. S. Corley, D. Heaton, D. Chatterji, J. C. Carver, and N. A. Kraft.

5.59898895385

====================
When the threshold is low, it is easy to acquire low-quality text and both approaches are the same.

5.59581884365

====================
Vader: A parsimonious rule-based model for sentiment analysis of social media text.

5.59541974323

====================
In ACM SIGecom Exchanges.

5.59193478867

====================
Note that the baseline does not acquire any high quality reviews when thresh >= 6.

5.59153826391

====================
However, Precog is more effective when the threshold increases.

5.58731950298

====================
These methods focus more on finding good contributors and lack content-specific feedback (e.g., discuss camera quality for a phone).

5.58240519998

====================
To contrast, we focus on using explicit constraints and ambiguous quality measures (for text) and provide explicit DDL statements to push them to the input interface.

5.58025570384

====================
[13] R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm, and N. Elhadad.

5.57884427635

====================
In the long term, we envision Precog as an example of automatically applying pre-hoc quality control (e.g., writing feedback) based on downstream application needs (e.g., quality reviews).

5.57629467515

====================
We also used document-quality labels to train the segment classifier.

5.56713951534

====================
Sections 4 and 5 surveyed work related to text quality prediction and writing feedback.

5.56640531934

====================
Asking the right questions in crowd data sourcing.

5.56180788808

====================
[47] S.-M. Kim, P. Pantel, T. Chklovski, and M. Pennacchiotti.

5.5599042851

====================
for host profiles.

5.55606236101

====================
[48] A. Kittur, B. Smus, S. Khamkar, and R. E. Kraut.

5.55245471196

====================
These can be integrated as feedback and interface customizations in Precog.

5.55232955039

====================
Individual differences in reading to summarize expository text: Evidence from eye fixation patterns.

5.54936038808

====================
In reality, there is often a long tail of topics without sufficient content for such approaches to be effective [61, 79].

5.54751601472

====================
They also assume a large corpus that contains high quality content for every topic (e.g., product or question).

5.54632996982

====================
For such cases, improving quality during user input process may be more effective.

5.5420474188

====================
Unanswered quora.

5.54160571216

====================
In UIST, 2011.

5.53961455663

====================
[27] FoxType.

5.53253902453

====================
[53] B. Letham, C. Rudin, T. H. McCormick, D. Madigan, et al.

5.52819519482

====================
The basic idea is to push data-quality constraints down to the data collection interface and improve data quality before acquisition.

5.52608968466

====================
8.

5.52233065656

====================
While the idea is easy to achieve for simple data types and constraints, it faces significant challenges for text documents.

5.51093643477

====================
[7] O. Biran and K. McKeown.

5.50830160675

====================
js Essentials.

5.49474688304

====================
[44] J. C. T. Ji-Wei Wu.

5.49061155133

====================
These ideas can be viewed as instances of Precog.

5.48733836508

====================
Linguistic analysis of chat transcripts from child predator undercover sex stings.

5.48430585526

====================
[37] I.

5.47907075205

====================
Moreover, none focus on multi-paragraph text attributes such as product reviews or forum comments.

5.47846796386

====================
American Psychological Association, 2002.

5.47613326593

====================
She, Y. Tong, and L. Chen.

5.47093153178

====================
Quality management on amazon mechanical turk.

5.4678887617

====================
Interacting with predictions: Visual inspection of black-box machine learning models.

5.46110004685

====================
MIT Press, 1997.

5.45695792354

====================
Scale-driven automatic hint generation for coding style.

5.45289741276

====================
In MSR, 2013.

5.44978369515

====================
Double data entry: what value, what price?

5.44246873422

====================
In EC, 2007.

5.43817923463

====================
Survey methodology, volume 561.

5.43309219756

====================
In Recommender Systems Handbook.

5.43276645336

====================
[33] A. Ghosh.

5.42950816525

====================
[12] C. C. Cao, J.

5.4180784272

====================
In WWW, 2010.

5.41342941281

====================
support.google.com/docs/answer/57859, 2016.

5.41150460362

====================
John Wiley & Sons, 2011.

5.41057964539

====================
React.

5.40896539967

====================
Fix reviews' grammar, improve sales.

5.40522126029

====================
Gene selection for cancer classification using support vector machines.

5.40305503623

====================
[14] A. Chalamalla, I. F. Ilyas, M. Ouzzani, and P. Papotti.

5.40182912804

====================
[5] E. Bakshy, B. Karrer, and L. A. Adamic.

5.39708843511

====================
Deriving the pricing power of product features by mining consumer reviews.

5.38973791993

====================
Automatically assessing review helpfulness.

5.38411765528

====================
Peerstudio: Rapid peer feedback emphasizes revision and improves performance.

5.38232598446

====================
Wiley e-rater Online Library, 2004.

5.37172408207

====================
Automated essay scoring with R v. 2.0.

5.3695308516

====================
Springer, 2015.

5.36464208186

====================
A. Kulik and C.-L. C. Kulik.

5.36412310819

====================
Latent dirichlet allocation.

5.36348095995

====================
[61] J. McAuley, R. Pandey, and J. Leskovec.

5.36293538198

====================
Springer, 2002.

5.36032329354

====================
In System Sciences (HICSS), 2011 44th Hawaii International Conference on, pages 1-10.

5.35810373586

====================
An efficient linear text segmentation algorithm using hierarchical agglomerative clustering.

5.35687573081

====================
Building reputation in stackoverflow: an empirical investigation.

5.35491436495

====================
[32] A. Ghose and P. G. Ipeirotis.

5.35455453991

====================
iCrowd: an adaptive crowdsourcing framework.

5.35320105418

====================
A survey of general-purpose crowdsourcing techniques.

5.34429395455

====================
In TKDE, 2011.

5.34158057168

====================
In Machine learning.

5.34042374347

====================
[35] Google.

5.33920261845

====================
[46] J. Kim.

5.33832738703

====================
Sage Publications, 2012.

5.33283530226

====================
https://www.amazon.com/gp/help/customer/display.

5.33241320872

====================
rfk/pyenchant, Jan 2011.

5.33025785348

====================
Guy.

5.32726179893

====================
[21] S. Deterding, D. Dixon, R. Khaled, and L. Nacke.

5.31694157013

====================
Justification narratives for individual classifications.

5.31517356112

====================
[57] X. Ma, J. T. Hancock, K. L. Mingjie, and M. Naaman.

5.31269377764

====================
In CIS, 2011.

5.30407660859

====================
Self-disclosure and perceived trustworthiness of airbnb host profiles.

5.30130787022

====================
Controlled clinical trials, 19(1):15-24, 1998.

5.29663884268

====================
[45] R. Kelly.

5.29517137073

====================
Respondable: Personal ai assistant for writing better emails.

5.28391633807

====================
In WSDM, 2008.

5.28367963804

====================
In ICDE, 2012.

5.27005952426

====================
Fan, G. Li, B. C. Ooi, K. Tan, and J. Feng.

5.2694734615

====================
In Media, Culture & Society.

5.26031391896

====================
[69] K. Norman, S. Lee, P. Moore, G. Murry, W. Rivadeneira, B. Smith, and P. Verdines.

5.25491604523

====================
VLDB, 2015.

5.25304673147

====================
In HCI, 2016.

5.25023912324

====================
Soylent: A word processor with a crowd inside.

5.24557552341

====================
In HCOMP, 2015.

5.23966717964

====================
Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model.

5.23933317632

====================
In SIGMOD, pages 445-456, 2014.

5.23712825361

====================
Social influence and the diffusion of user-created content.

5.22866180502

====================
[42] P. Ipeirotis.

5.22552077798

====================
html?nodeId=201929730, 2016.

5.219342952

====================
[3] N. Archak, A. Ghose, and P. G. Ipeirotis.

5.21883264597

====================
In Review of educational research, 1988.

5.2165420222

====================
jury selection for decision making tasks on micro-blog services.

5.21625694054

====================
Lin, Y. Huang, and M. Zhou.

5.21137895386

====================
In Management Science.

5.2030478801

====================
INFORMS, 2011.

5.19967672672

====================
In JMLR, 2003.

5.19595818882

====================
[22] P. K. Dick, S. Spielberg, T. Cruise, and S. Morton.

5.19481735203

====================
[20] S. Day, P. Fayers, and D. Harvey.

5.19082176165

====================
Amazon: Community guidelines.

5.17883811221

====================
[15] C. C. Chen and Y.-D. Tseng.

5.17632448896

====================
In ACL, 2006.

5.17112941106

====================
[16] A. I. Chittilappilly, L. Chen, and S. Amer-Yahia.

5.16667627195

====================
Finding high-quality content in social media.

5.1637805056

====================
Journal of Language and Social Psychology, 35(4):435-445, 2016.

5.1570735316

====================
[17] R. R. Choudhury, H. Yin, and A.

5.14931981327

====================
In AutoML, 2014.

5.14800784631

====================
[50] M. Krause.

5.13709941684

====================
a multi-method approach to determine the antecedents of online review helpfulness.

5.13119274568

====================
[10] Boomerang.

5.12316291219

====================
http://www.boomeranggmail.com/respondable/, 2016.

5.12204318217

====================
[19] P. B. Darwin and P. Kozlowski.

5.11377095843

====================
[2] Amazon.

5.11189745277

====================
[65] H. Misra, F. Yvon, O. Cappe, and J. Jose.

5.10096438976

====================
From game design elements to gamefulness: defining gamification.

5.08858233516

====================
Whom to ask?

5.08858128107

====================
Minority report.

5.0606588256

====================
[76] M. T. Ribeiro, S. Singh, and C. Guestrin.

5.05818816377

====================
In EC, 2009.

5.0523383588

====================
Deco: declarative crowdsourcing.

5.0414364022

====================
In UIST, 2010.

5.03429145815

====================
[52] C. E. Kulkarni, M. S. Bernstein, and S. R. Klemmer.

5.01363421879

====================
Descriptive and prescriptive data cleaning.

5.00996399008

====================
In Decision Support Systems.

5.00916634272

====================
Human-powered sorts and joins.

5.00644857267

====================
In ITS, 2016.

4.99877342391

====================
In Learning Research and Development Center, 2007.

4.99253779092

====================
Timing of feedback and verbal learning.

4.98793411158

====================
Packt Publ., 2013.

4.96782395254

====================
Low-quality product review detection in opinion summarization.

4.96466459936

====================
[55] J. Liu, Y. Cao, C.-Y.

4.94292553909

====================
Quality evaluation of product reviews using an information quality framework.

4.93792198616

====================
In MindTrek, 2011.

4.93607964229

====================
PVLDB, 2012.

4.93570537853

====================
[58] N. Madnani and A. Cahill.

4.92609585412

====================
http://www.imdb.com/title/tt0181689/, 2002.

4.9237190498

====================
An explicit feedback system for preposition errors based on wikipedia revisions.

4.92161621164

====================
[82] S. Siersdorfer, S. Chelaru, W. Nejdl, and J. San Pedro.

4.90746912086

====================
Textblob: Simplified text processing.

4.89757849657

====================
Inferring networks of substitutable and complementary products.

4.89311661095

====================
Linguistic obfuscation in fraudulent science.

4.88561791967

====================
[66] L. Muchnik, S. Aral, and S. J. Taylor.

4.86978460963

====================
Is it the review or the reviewer?

4.86773302784

====================
The Annals of Applied Statistics, 2015.

4.85911008507

====================
Fox.

4.85155574909

====================
IEEE, 2011.

4.83248980379

====================
AngularJS web application development.

4.827935804

====================
In EMNLP-CoNLL, 2007.

4.78751991466

====================
[56] S. Loria.

4.78728756056

====================
https://textblob.readthedocs.io/en/dev, 2014.

4.78317698881

====================
In Advances in neural information processing systems, pages 121-128, 2008.

4.77300271672

====================
In CSCW, 2017.

4.75124708691

====================
[70] O. Nov. What motivates wikipedians?

4.75023953486

====================
Winning arguments: Interaction dynamics and persuasion strategies in good-faith online discussions.

4.74920602549

====================
[59] A. Marcus, E. Wu, D. Karger, S. Madden, and R. Miller.

4.74768413108

====================
In MIS quarterly, 2010.

4.74099147043

====================
[68] M. Nelson and C. Schunn.

4.73502190072

====================
[60] D. M. Markowitz and J. T. Hancock.

4.72577297861

====================
[24] J.

4.71915792372

====================
13  [51] J.

4.70247589602

====================
[62] J. D. Mcauliffe and D. M. Blei.

4.68549474909

====================
Check spelling and grammar in office 2010 and later.

4.66756526745

====================
Towards globally optimal crowdsourcing quality management: The uniform worker setting.

4.66455605691

====================
In L@S, 2015.

4.66429494353

====================
[73] J. W. Pennebaker, R. L. Boyd, K. Jordan, and K. Blackburn.

4.66343733283

====================
The development and psychometric properties of liwc2015.

4.65138574501

====================
support.office.com, 2016.

4.63990688219

====================
[64] B. L. Minqing Hu.

4.62707662724

====================
Technical report, 2015.

4.61728046782

====================
Supervised topic models.

4.61637067019

====================
The nature of feedback: Investigating how different types of feedback affect writing performance.

4.59526815373

====================
American Association for the Advancement of Science, 2013.

4.58237030259

====================
In Computational Linguistics.

4.56711553463

====================
Topictiling: a text segmentation algorithm based on lda.

4.56179310491

====================
[77] M. Riedl and C. Biemann.

4.5616775739

====================
[75] R. S. Rakesh Agrawal.

4.55334049915

====================
In NAACL, 2014.

4.53754988259

====================
[78] K. Rivers and K. R. Koedinger.

4.53516507787

====================
Elsevier, 2011.

4.53287769629

====================
What makes a helpful review?

4.53067289321

====================
[63] Microsoft.

4.52286636627

====================
VLDB, 2011.

4.50859820098

====================
QASCA: A quality-aware task assignment system for crowdsourcing applications.

4.50691940744

====================
[89] S. Valenti, F. Neri, and R. Cucchiarelli.

4.49399988177

====================
In Information Processing & Management.

4.48271066045

====================
CrowdFill: collecting structured data from the crowd.

4.47608769794

====================
In KDD, 2015.

4.46257985608

====================
In CIKM, 2012.

4.45532692349

====================
Social influence bias: A randomized experiment.

4.43455127716

====================
[74] L. Pevzner and M. A. Hearst.

4.41578988856

====================
[83] R. Singh, S. Gulwani, and A. Solar-Lezama.

4.41268051053

====================
Mining opinion features in customer reviews.

4.41258774526

====================
In AAAI, 2004.

4.39488762887

====================
Text segmentation: A topic modeling perspective.

4.39121234698

====================
[94] M. N. Xiao Ma, Trishala Neeraj.

4.37482446209

====================
[85] C. Tan, V. Niculae, C. Danescu-Niculescu-Mizil, and L. Lee.

4.35943307965

====================
": Explaining the predictions of any classifier.

4.34191437124

====================
[67] S. M. Mudambi and D. Schuff.

4.33231373149

====================
a study of customer reviews on amazon.com.

4.31627805345

====================
Fast algorithm for mining association rules.

4.315057476

====================
Automating hint generation with solution space path construction.

4.31495818667

====================
In Science.

4.31457745354

====================
[87] B. Trushkowsky, T. Kraska, M. J. Franklin, and P. Sarkar.

4.29958173331

====================
In Communications of the ACM, 2007.

4.29745640858

====================
A critique and improvement of an evaluation metric for text segmentation.

4.28214985932

====================
What in the hay is a zappos premier reviewer?

4.28170168761

====================
improving data quality and data mining using multiple, noisy labelers.

4.2779561622

====================
quora.com/What-percentageof-questions-on-Quora-have-no-answers, 2016.

4.25513667697

====================
In ICITS, 2014.

4.25135245595

====================
[79] G. Saito.

4.24891113627

====================
Online survey design guide, 2003.

4.244094509

====================
Supersparse linear integer models for optimized medical scoring systems.

4.24209946636

====================
[90] G. Wang, K. Gill, M. Mohanlal, H. Zheng, and B. Y. Zhao.

4.20820300182

====================
[81] V. S. Sheng, F. Provost, and P. G. Ipeirotis.

4.19761884641

====================
[72] H. Park and J. Widom.

4.19526132635

====================
[80] A. D. Sarma, A. G. Parameswaran, and J. Widom.

4.16217619843

====================
In SIGMOD, 2014.

4.15030804478

====================
Crowdsourced enumeration queries.

4.14773807801

====================
How useful are your comments?

4.1409780441

====================
List of nfc phones.

4.12332169032

====================
en.wikipedia.org/wiki/Wikipedia: Policies_and_guidelines, 2016.

4.12200825908

====================
In VLDB, 1994.

4.08186910097

====================
[84] N. Spirin and J. Han.

4.07984378105

====================
MIT Press, 2002.

4.06786549644

====================
Technical report, Stanford InfoLab, 2012.

4.05536630494

====================
"why should I trust you?

4.00644793125

====================
In SIGKDD, 2016.

3.98113054244

====================
An overview of current research on automated essay grading.

3.9490917841

====================
In Journal of Information Technology Education, 2003.

3.94869937749

====================
Recommending the world's knowledge: Application of recommender systems at quora.

3.94508794239

====================
In ACL, 2012.

3.94391851962

====================
In SNAM.

3.94057030013

====================
preprint maxiao.info, 2017.

3.93356438741

====================
[88] B. Ustun and C. Rudin.

3.90906802694

====================
: analyzing and predicting youtube comments and comment ratings.

3.8662624876

====================
Compare me maybe: Crowd entity resolution interfaces.

3.85743086386

====================
Wisdom in the social crowd: an analysis of quora.

3.8299408593

====================
Get another label?

3.82304296652

====================
In SIGMOD, 2016.

3.81503452821

====================
[91] S. E. Whang, J. McAuley, and H. Garcia-Molina.

3.79893460996

====================
[86] J. Tang, X. Hu, and H. Liu.

3.79445834126

====================
Automated semantic grading of programs.

3.79322212891

====================
[95] L. Yang and X. Amatriain.

3.77938933606

====================
Survey on web spam detection: principles and algorithms.

3.7774953992

====================
In KDD, 2008.

3.74060040203

====================
Editor guidelines.

3.6893576916

====================
Technical report, MIT, 2012.

3.65894396698

====================
https://www.nfcworld.com/nfc-phones-list/, 2016.

3.62416300606

====================
In KDD, 2012.

3.56605328338

====================
Social recommendation: a review.

3.52661472681

====================
In WWW, 2016.

3.50339360096

====================
[92] Wikipedia.

3.48340604973

====================
In RecSys, 2016.

3.48015780981

====================
yelp.com/guidelines, 2016.

3.43700636867

====================
Springer, 2013.

3.42526589502

====================
http://www.zappos.com/premier-reviewers, 2016.

3.39444430572

====================
In ICDE, 2013.

3.35822714832

====================
Machine Learning, 2016.

3.33633895572

====================
Content guidelines.

3.24356770002

====================
[98] Y. Zheng, J. Wang, G. Li, R. Cheng, and J. Feng.

3.16316540149

====================
In WWW, 2013.

3.09778837182

====================
[93] N. World.

2.99501030489

====================
A computational approach to perceived trustworthiness of airbnb host profiles.

2.91837640491

====================
[96] Yelp.

2.36143859981

====================
[97] Zappos.

2.3206851094

====================
In SIGMOD, 2015.

1.77491724569

====================
14

1.44159863921

None
