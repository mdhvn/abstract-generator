Title:  PreCog: Improving Crowdsourced Data Quality

['precog:', 'improving', 'crowdsourced', 'data', 'quality']
====================
To this end, we present Precog1 , a crowdsourced data acquisition system that supports pre-hoc quality control for both simple data types and multi-paragraph text attributes.

18996.386174

====================
We further show that Precog's unique approach to combining prescriptive explanations and segment-level feedback improves text quality by 14.3%, and over 3x better than a state-of-the-art feedback system [50].

13214.109921

====================
To build high-quality models, we survey and categorize features from the writing analysis literature into 5 categories  1. user input  2. segment by topic  3. estimate quality  4. targeted feedback  Figure 3: The Segment-Predict-Explain pattern: Precog splits user input into coherent segments; estimates the quality of each segment and the text as a whole; and generates and shows suggested improvements to the user.

7068.80508176

====================
The fourth FEF for product reviews was mapped to Subjectivity features in (Table 1) and the fourth host profiles FEF was mapped to Friendliness LIWC features shown in [94], with each returning text suggesting that the user improves the respective facet of their submission (i.e., "Please make your writing more balanced and neutral").

6418.45015477

====================
Experimental Conditions: The purpose of experiments is to both show the Cost Saving benefits of Precog as well as to evaluate the effectiveness of it's two main features (segment-level feedback and TCruise explanation generation).

6333.96471664

====================
NEW APPLICATION DOMAINS  EXPERIMENTS  We now evaluate how Precog improves high-quality data acquisition using live Mechanical Turk deployments.

6085.3822113

====================
Our experiments and prior work [52] show that this is less effective than a more customized approach.

5215.35197965

====================
For explanation functions, prior work showed that 75% of reasons for unhelpful reviews were covered by (in priority order) overly emotional/biased opinions, lack of information/not enough detail, irrelevant comments, and poor writing style [18].

4932.75421601

====================
In addition to evaluating Precog for hard constraints and simple data types, we evaluate Precog's text feedback through extensive MTurk experiments on two real application domains--product reviews and rental host profiles.

4771.41086427

====================
Through extensive MTurk experiments, we find that Precog collects >= 2x more high-quality documents and improves text quality by 14.3% compared to not using pre-hoc techniques.

3598.53198219

====================
In this paper, we argue for pre-hoc quality control systems.

3285.04374244

====================
We directly address this problem by selecting multi-feature explanation functions to prescribe improvements to the user's text.

3227.74347968

====================
The first category, Informativeness, highlights trends across existing literature that show that both general length measures [4, 50, 55, 57, 82] as well as domain-specific jargon are highly predictive of quality [55, 57, 60].

2797.33618492

====================
To this end, we performed a survey of literature spanning of social media text analysis [32, 55, 57, 82, 85], essay grading [4,25,58,89], deception detection [23,60], and information retrieval [64, 84].

2495.0405808

====================
We validated generalizability of the model to domains not covered in prior work by evaluating it on reddit comments from the AskScience subreddit5 and predicted comment helpfulness on an evenly balanced sample with 80% accuracy6 .

2294.53339308

====================
We showed this to be sufficient by testing on crowd-sourced labels; however more sophisticated techniques to classify segments could improve feedback.

2210.37569613

====================
Furthermore, while most approaches simply use the distribution of topics as a feature [57, 62], Precog computes several summary statistics (entropy, topic ID and probability of top-K topics, ranked by probability) not used in prior work that prove highly predictive in our experiments.

2170.87001168

====================
For example, task replication [43, 80] assigns the same task to multiple workers and aggregates them into a single result; multi-stage workflow design [6, 48] uses additional crowd tasks to (iteratively) refine previously submitted tasks; in text acquisition, filtering/ranking [1, 37, 67, 82, 84, 86, 90, 95] uses crowd tasks to assess each document's quality and either rank them by quality or filter out low-quality documents.

1960.52481497

====================
We believe our assumption about the availability of a training corpus is reasonable in data acquisition settings, because such quality labels are already gathered in order to rank documents (e.g., Amazon helpful/unhelpful reviews, Reddit comment up/down votes).

1892.56541507

====================
Precog, which is complementary to post-hoc quality control techniques, collects >= 2x high-quality documents for the same budget as no feedback, and improves text quality by 14.3% on average.

1730.71682285

====================
All participants were US Residents with > 90% HIT accept rates.

1659.30142521

====================
def offTopic(topics, text="", feats=[]): if len(topics) < 5: sortedTopics = sorted(topics, key=topic.prob) return "Try discussing some of these topics: " + topK(sortedTopics, 5)  We used a similar process for host profiles and found that research emphasizes trustworthiness as the key quality metric [57, 94].

1583.1181331

====================
We then computed pairwise accuracies between the document labels, classifier predictions, and crowd labels: 71.1% (Classifier predicting Crowd Label), 72.5% (Classifier predicting Document label), and 69.5% (Document label predicting Crowd Label).

1563.41296906

====================
These plots show the effect size across all measures, and that the largest improvements were due to the combination of segmentation and TCruise-based explanation.

1548.51135249

====================
We describe our experiments that show that it is possible to use these labels as a proxy for individual segments.

1533.18234485

====================
def notEnoughDetail(topics, featureCnt, textLen, text="", feats=[]): if featureCnt < 10 and textLen < threshold: return "Try adding information about: " + suggest_new_prod_feats(topics, text, feats) ...  We note that existing feedback systems [7,49,50] implicitly follow this model, however they bind individual features to static strings.

1491.26377418

====================
Fp is the subset of features that p perturbs:  Sfd  We are now ready to present the key technical problem for text acquisition feedback:  5.4  U (vij ) - U (M (d))  Our implementation indexes all paths in the random forest by their utility.

1477.79356041

====================
The consistent results between all three comparisons suggest the efficacy of the segment-level classifier, and our end-to-end experimental results suggest that the predictive model is effective at providing segment level feedback.

1418.27431904

====================
Although crowdsourcing is used to collect labels and simple data for machine learning applications, many popular online communities such as Amazon, AirBnB, Quora, Reddit, and others also rely on collecting and presenting high quality, open-ended content that is crowdsourced from their users.

1407.56430429

====================
count) LDA topic distribution and top topics [8], entropy across topic distribution opinion sentence distribution stats [64], valence, polarity, and subjectivity scores and distribution across sentences [32, 34, 56], % upper case characters, first person usage, adjectives spelling errors [45], ARI, Gunning index, Coleman-Liau index, Flesch Reading tests, SMOG, punctuation, parts of speech distribution, lexical diversity measures, LIWC grammar features various TF-IDF and top parts of speech comparisons with sample of low and high utility documents  Table 1: Summary of feature library for text quality.

1378.59413545

====================
To understand the contributing factors towards the quality improvements, we compared four feedback systems that varied along two dimensions: granularity varies the feedback to be at the document level (Doc), or at the document and segment level (Seg); explanation selection compares the single-feature outlier technique from [50] (Krause) with TCruise.

1259.2107224

====================
In this section, we first describe how users express Precog quality control for common data integrity constraints, as well as quality scores on a crowd-sourced table.

1237.32878126

====================
When we consider a user's edits, they are desirable if the edits will improve the document's quality--in other words, if it will cause the document to be reclassified as high quality.

1221.88256689

====================
To this end, we define the responsibility Sfdi of a feature fi for input point d as the sum of all maximum influence perturbations that involve fi (e.g., the perturbation pi 6= 0): X  Sfdi =  I(d, p)  I(d, qij ) =  p[?

1191.18125232

====================
To ensure fair comparison, we supplemented their features with domain-specific features for Informativeness (# of product features/jargon), Readability (Coleman-Liau index), and Friendliness (LIWC features related to friendliness) so that their features are comparable to those used in our feature library.

1131.17803817

====================
For example, Amazon crowdsources product reviews by asking customers to rate products and write reviews for them; rental services (e.g., AirbnB) relies on rental hosts to describe their rental properties in quantitative (e.g., number of bed rooms, wireless) as well as qualitative terms (e.g., textual description).

1110.14557359

====================
To summarize our contributions: * We present the argument for pre-hoc quality control and present its unique advantages as well as the challenges for multi-paragraph text.

1086.74687635

====================
Second, we evaluate Precog's Segment-Predict-Explain pattern for text acquisition in two domains--acquiring customer reviews for Amazon products [61] and acquiring profile descriptions for AirBnB host profiles [94].

1068.42690363

====================
def numeric_exp(att, val, err): return "%s: '%s' should be a number" % (att, val) CREATE EXPLANATION ON reviews(rating) FOR reviews_rating_domain USING numeric_exp; CREATE EXPLANATION ON users(age) FOR users_age_domain USING numeric_exp;  Similar functions can easily be written for the foreign-key and uniqueness constraints in Figure 4: def product_exp(att, val, err): return "%s is not a product" % val def unique_exp(att, val, err): return "%s has been taken" % val  For text attributes, the explanation function is slightly different, which is defined on a FEATURE table.

1067.42445014

====================
To this end, we use a technique called TopicTiling [77], an extension to TextTiling [40].

1063.19302478

====================
The prior work predicts the quality of Amazon DVD, AV player and Camera reviews with 83% accuracy; Precog's default model on the same setup predicts at 85% accuracy--the slight improvement is due to the additional features in the topic and similarity categories from other literature (Table 1).

1031.39967877

====================
The problem is exponential and we present an efficient solution that leverages the structure of random forest models to generate high-quality feedback in interactive time.

1015.91817793

====================
* Extensive MTurk experiments on two real-world domains with different quality measures: helpfulness for Amazon product reviews and trustworthiness for AirBnB housing profiles.

1007.94141896

====================
We first describe our extensible feature library that consolidates text features across literature in social media text analysis, essay grading, language psychology, and data mining research communities.

984.390189348

====================
We describe our process to extend Precog to two domains with different quality measures: product reviews that care about helpfulness to a shopper [3], and then host profiles that are judged by trustworthiness to renters [57].

972.109064062

====================
In contrast, Precog supports feature combinations and can dynamically generate feedback based on the  Responsibility: Our goal is to identify feature subsets of the test data point d that, if perturbed, will most improve 7  d's utility7 .

961.451950542

====================
We first present the results of the fully featured Precog condition (Section 7.2.1) and then demonstrate the contribution of each Precog component (in Section 7.2.2).

937.344568936

====================
Finally, we asked coders to subjectively rate their agreement from 1-7 to the statement "The post-feedback revisions improved on the pre-feedback document.

931.67936955

====================
2.1  Pushing Data Constraints to the Interface  Precog extends existing crowdsourced databases that contain crowdsourced and non-crowdsourced base relations; a crowdsourced table [28] represents a subset of all possible records that may be stored in the table and the task is to acquire records to insert into the table.

899.050630553

====================
5.3  Problem Statement  Intuition: Figure 6 depicts the main intuition behind the problem and our approach.

894.860314351

====================
Quality scores are intended for attribute values for which the definition of quality defined as a continuous measure to be improved, rather than a boolean constraint, and provides the framework for which we implement a model-based feedback system for performing Precog on text attributes (Section 3).

855.392454546

====================
Krause [50] was shown to outperform static explanations of important components of a helpful review (similar to a rubric) for students performing peer code-reviews and uses an outlier based approach described in Section 5.1.

843.166042632

====================
7.2  Precog for Text Acquisition  Setup and Datasets: Precog is setup as described in Section 6: we train Precog using the laptop category of the Amazon product reviews corpus [61], and the AirBnb profile corpus [94] for their corresponding experiments.

842.646173235

====================
CONCLUSION AND FUTURE WORK  This paper presented the design, implementation and evaluation of Precog, a pre-hoc quality control system.

837.113614174

====================
Precog: A PRECOG SYSTEM  As described in the introduction, Precog seeks to optimize the data collection interface in order to improve the quality of the collected data and ensure data quality constraints.

812.377355193

====================
Most studies focus on post-hoc quality control, often using additional crowdsourced tasks to assess and improve the quality.

792.725044679

====================
However, the combination of segmentation and TCruise consistently produced larger effect sizes than all other conditions across both Host Profiles and Product Reviews: for Product Reviews Precog, which combines segmentation and TCruise, improved the overall measure (bottom left facet) by nearly 3.9x over the baseline (0.55 vs. 0.14 increase), and a 2.4x improvement over the next-best Doc+TCruise condition.

787.61414438

====================
As illustrated in Figure 3, we employ a novel SegmentPredict-Explain pattern to generate customized feedback on an individual segment (rather than document) level.

764.443434785

====================
We define this as the Prescriptive Explanation problem, and find that the search space of solutions for the problem is exponential in the number of model features.

763.218273597

====================
In summary, we find that TCruise is essential to improving document quality; combining TCruise with Segmentation empirically produces the best results across the board.

758.535189054

====================
Product-Review Participants: For the laptop review experiment, we recruited 85 workers on Amazon's Mechanical Turk (61.2% male, 38.8% female, ages 20-65 uage =32, sage =8.5).

729.813833665

====================
We first introduce the Prescriptive Explanation problem, which assigns responsibility to each model feature proportional to the amount that it will contribute to improving the predicted text quality.

721.537778452

====================
Furthermore, controlling for the other variable, TCruise showed a statistically significant difference in improvement, while segmentation did not.

717.504077775

====================
Host-Profile Participants: For the profile description experiment, we recruited 92 workers on Amazon's Mechanical Turk (58.7% male, 41.3% female, ages 20-62 uage =33, sage =8.2); all completed the task.

716.745502425

====================
A developer first defines an explanation function that takes as input the list of attribute names and values for which the constraint is defined for (in order to support multi-attribute constraints) and the error message, and returns a string that is shown as feedback.

714.288050106

====================
To address this issue, we present a Segment-Predict-Explain pattern that  Architecture: Figure 5 depicts the system architecture.

696.263736162

====================
First, we validate the value of pre-hoc quality control by running a crowdsourced data acquisition experiment with different Precog optimizations for foreign key and domain constraints.

688.943748213

====================
Readability/Grammar is an aggregate of syntactic features shown predictive across multiple domains [23, 50, 60, 82].

688.406117983

====================
This problem is challenging because we must analyze potentially arbitrary text content.

685.29379791

====================
Each rubric rated documents on a 1-7 Likert scale using three specific aspects identified by prior work--Informativity, Subjectivity, Readability--for reviews--Ability, Benevolence, Integrity-for profile trustworthiness, as well as a holistic overall score.

667.26032538

====================
For this, we use TopicTiling [77], a sliding window-based segmentation algorithm that computes the dominant topics within the window using LDA [8].

664.898127549

====================
Specifically, we develop effective approaches to measure text quality at both document and segment levels, present an efficient technique to solve the prescriptive explanation problem, and discuss how to extend Precog to new domains.

628.506746346

====================
We compute a variety of similarity measures between the input document and a sample of high and low quality documents-using both the simple TF-IDF measure used in prior work [47] as well as occurrences of popular parts of speech appearing in a document (i.e top-K nouns in unhelpful documents that appear).

625.138229297

====================
8  We added LIWC API calls to Precog; the model tested on a balanced set of 300 AirBnB host profiles was competitive (79% accuracy) at predicting if a profile was >= median trustworthiness.

622.112512584

====================
We then performed pairwise Tukey HSD post-hoc tests between each of the four conditions.

619.538963335

====================
It can be integrated seamlessly into existing crowdsourcing applications or systems with post-hoc quality control, helping them to further improve quality.

614.809207858

====================
Our contribution is to curate the subset of these features that can be generalized across text domains to improve writing quality, categorize them (Table 1), and integrate them into an open source feature library4 .

604.623911411

====================
We address these challenges by proposing a novel segment-predict-explain pattern for detecting lowquality text and generating prescriptive explanations to help the user improve their text.

592.205038896

====================
Since good feedback can help the worker improve the text, it naturally improves the quality of the acquired data, and can reduce data acquisition costs.

590.241368472

====================
Specifically, we ran a crowdsourced study to label 500 Amazon segments (250 drawn from helpful reviews, and 250 from unhelpful reviews), with human helpfulness labels (the median segment length of a review is 3).

569.495697372

====================
We then randomly assigned each worker 50 segments to label, and collected labels until each segment had >= 3 labels, and determined the final label of each segment using the Get Another Label algorithm [81].

569.135790349

====================
Cost Savings  Figure 10: Improvement on Likert scores for both domains (reviews and profiles) and four quality criteria per domain.

566.322154513

====================
These statements complement existing task interface specifications that prior crowdsourcing systems [28, 59, 71] use for task generation by providing a way to augment them for data integrity constraints.

565.600688848

====================
def exp_func(att1, val1, ..., attn, valn, err=None): return "custom error message" CREATE EXPLANATION <func> ON <table>(<att1>,..<attn>) FOR <CONSTRAINT NAME> USING <explanation function>;  Below is the specification to customize the feedback for a numeric domain constraint3 .

550.882787273

====================
We are optimistic about the Segment-Predict-Explain pattern, because adopting to new domains is simply a matter synthesizing existing research by adding features and creating simple explanation functions.

550.421586589

====================
Below, we describe how developers can express the three levels of Precog quality control for domain, foreign-key, uniqueness, and quality score constraints in Figure 4.

546.575861408

====================
Precog takes long form text from a crowd worker, decomposes it into coherent portions (segments) based on their topics, predicts the quality of each segment, and automatically generates immediate feedback to explain how these segments can be improved.

541.106946686

====================
We implement a variety of length measures, and use the Apriori algorithm [75] to mine jargon based on the training data inputted into Precog, and identify its distribution across the sentences of an input document.

540.136134537

====================
To do so, we first define the impact I(d, p) for an individual perturbation p as the amount that it improves the utility function discounted by the amount of the perturbation [?

539.103835078

====================
For this, we next introduce DDL statements to specify custom interfaces.

534.127064806

====================
CREATE CROWD TABLE users ( id autoincrement primary key, username text UNIQUE, age int CHECK age > 0 AND age < 100, CHECK(username matches \w+) ); CREATE CROWD TABLE reviews( id autoincrement primary key, product_id text, rating int CHECK rating > 0 AND rating <= 5, review text, QUALITY SCORE qualreview qual_udf(review), FOREIGN KEY product_id REF products(id) ); CREATE FEATURE TABLE review_feats( review text primary key references reviews.review, topics FEATURE topic_extractor, len FEATURE len_extracton, ... );  Generic Feedback: Precog automatically generates feedback based on the error message that the underlying database generates when the INSERT violates a constraint.

526.504965147

====================
In future work, we hope to explore a broader range of applications (e.g., different social media domains or user contexts), and study how to optimize data-collection interfaces to meet more complex application needs.

513.838013443

====================
In this work, we restrict the analysis to perturbations that have the maximal influence.

491.075353856

====================
We choose a random forest classifier, which has been shown effective in existing work [32], and select features using the recursive feature elimination algorithm [38].

490.356085813

====================
The only change is the beige component, which augments the data collection interface (task interface) to estimate the quality of the user's (in this case) text, and automatically provide feedback if the predicted quality is low.

486.815105799

====================
This results in a 2x2 between-subjects design.

485.464310802

====================
Our design is informed by the writing analysis and feedback literature, which emphasizes the value of providing immediate feedback [51], as well as finegrained feedback for specific portions of the text [17, 78, 83], as is common in coding environments.

471.912359375

====================
Upon pressing the I'm Done Writing button, the interface displayed our document-level feedback under the text field; for users in the segmentation condition, low quality segments were highlighted red and the related feedback displayed when users hovered over the segment.

471.204113727

====================
Precog also achieves 79% accuracy at predicting if an Airbnb profile is above or below median trustworthiness, using trustworthiness data from [94].

463.870791314

====================
As compared to other features libraries such as LIWC, Precog's main advantage is a high-concentration of data-driven features (topic modeling,  4  5  Available at http://cudbg.github.io/Dialectic  Category Informativeness Topic  # 8 5  Subjectivity  15  Readability and Grammar  15  Similarity  4  Description mined jargon word and named entity stats [64], length measures (word, sentence, etc.

444.867199653

====================
Figure 1 illustrates a typical text acquisition workflow: the crowd generates text documents, more tasks are used to estimate the text quality, low-quality documents are removed, and this may ultimately trigger the need to collect more data.

439.648853305

====================
Quality Control in Crowdsourcing: Quality control is an important research topic in crowdsourced data management [16, 30, 54].

424.595624939

====================
A dominant use case for crowdsourcing is to collect data-- labels, opinions, text extraction, ratings--from large groups of workers.

423.764542834

====================
Thus, three of the FEFs, Informativeness, Topic, and Readability/Grammar, overlapped between the two domains.

419.785203981

====================
However, recent study shows the promise of translating semantic features to textual feedback [50].

418.804289698

====================
Nevertheless, more studies are needed to fully evaluate this hypothesis across other text domains and document lengths.

406.542683261

====================
Finally, we perform a detailed analysis of how segmentation and TCruise each contribute to improving the quality of the acquired text.

405.157350197

====================
In this section, we describe our approach towards in-depth semantic feedback.

399.968416191

====================
F |pi 6= 0}  Finally, Sfdi computes the responsibility for fi as the sum of all maximal influence paths in all decision trees that improve the predicted utility U ().

398.509407809

====================
We then synthesized existing research to write 4 explanation functions for each domain, with 3 overlapping between the two.

396.992676402

====================
Pre-hoc methods improve quality before the data is acquired (submitted); Post-hoc methods improve quality after data acquisition (i.e., after submission).

396.168854464

====================
* We define the Prescriptive Explanation Problem to provide actionable feedback for text acquisition.

389.697123098

====================
Pre-hoc quality control occurs before data acquisition and naturally complements many existing post-hoc techniques to further improve the final data quality.

382.963363427

====================
Figure 11 shows a similar chart for the coder's subjective opinion of the improvement.

382.782146962

====================
Although it's possible to automatically perform pre-hoc quality control for simple constraints over simple data types, it is still unclear how this can be achieved for more complex data integrity constraints and data types.

381.342023497

====================
We then normalize a feature's responsibility Sfdi by computing Snormdfi =  d Sf -uf  i  i  sf  .

378.41707748

====================
To do so, we develop a novel perturbation-based analysis to identify the combination of features that, when changed, will most likely reclassify the text as high quality.

360.49067746

====================
A detailed explanation of the four conditions is shown in Section 7.2.2.

353.935010997

====================
We learn this quality measure by training a random forest model that predicts the quality of individual text segments.

350.022273294

====================
We then sort the FEFs by their average scores and take the top k with a score above the threshold t.  7.

346.981009537

====================
Existing approaches (surveyed in Related Work) focus on syntactic errors such as grammatical mistakes, which cannot help improve the text content, or overly simple models for picking feedback text [50].

344.77891741

====================
I(d, p) =  We instead present a heuristic solution called TCruise whose complexity is linear in the number of paths in the random forest model.

343.658827077

====================
We now describe related work in terms of data acquisition interface optimizations, quality control in crowdsourcing and other post-hoc quality mechanisms specific for text acquisition.

342.941259866

====================
Companies (e.g., Amazon, Zappos) use this post-hoc technique by asking users to assess whether a product review is "helpful" or "not helpful", and ranks and displays reviews based on this measure.

338.934303979

====================
In contrast, we generate prescriptive, actionable explanations that, if followed, are expected to improve the text.

332.472201851

====================
We then gave participants the opportunity to revise their submission; to avoid bias, we noted that they were not obligated to.

324.906200263

====================
For both Product Review and Host Profiles, we performed Two-Way ANOVAs with both the Overall Quality Im11  knowledge, Precog is the first system that systematically supports Precog for a wide range of data types and quality specifications (constraints and quality scores).

323.582338133

====================
Precog is easily extended to new domains, and increases the number of high-quality documents by >= 2x compared to not using pre-hoc techniques.

319.941327397

====================
Post-hoc Approaches for Text Acquisition: A dominant approach is to filter poor content [84] such as spam; sort and surface higher quality content [1, 37, 86] such as product reviews [67], answers to user comments [90, 95], or forum comments [82]; or edit user reviews for clarification or grammatical purposes [6, 42, 48].

319.501835793

====================
The rest of this subsection describes the DDL statements that users can use to specify feedback and interfaces for Precog quality control.

313.936580808

====================
Overview: In contrast to naive form validation, which simply rejects user inputs with an error message, Precog seeks to accommodate iterative improvements through feedback interfaces.

310.917765352

====================
This provides the functionality for our automatic pre-hoc quality control system for free-form text attributes.

300.134836681

====================
Protocol and Rubric for Assessing Quality: Three independent evaluators (non-authors) coded the pre and post-feedback documents using a rubric based on prior work on review quality [18, 55, 67] and Airbnb profile quality [57].

299.760838472

====================
The third coder was trained by being shown the Amazon or Airbnb corpus, examples across the quality spectrum, and the other two coders.

297.905843289

====================
Our efficient solution called TCruise leverages the structure of random forest models to generate explanations in interactive time.

297.207688424

====================
Our goal is to provide the foundation for such content-specific semantic feedback by surveying and categorizing features from the writing analysis literature.

295.75777017

====================
One solution is 5 6  EXPLAIN  5.1  Problem Background  Our problem is closely related to model explanation, which generates explanations for a model's (mis-)prediction.

291.876117561

====================
The document-level feedback is shown to the user, and the low-quality segments are highlighted as light red in the interface.

290.873390076

====================
Additionally, our Segment-Predict-Explain pattern addresses on free-form text entry that complements their focus on simple data types.

287.446891902

====================
Procedures: Participants writing product reviews were asked to write a review of their most recently owned laptop computer "as if they are trying to help someone else decide to buy that laptop or not and are writing on a review website like the Amazon store".

287.211558268

====================
We found that combining segmentation and TCruise-based explanation outperformed all other conditions by a statistically significant margin for Product Reviews, and outperformed all but the next-best Doc+TCruise condition for Host Profiles.

286.852172448

====================
We now formally define these feature-oriented explanation functions (FEFs) and provide examples used in the experiments.

286.391412856

====================
We conduct statistical tests to further investigate the results.

286.268197604

====================
Segmentation: Contributor rubrics across many social media services are structured around topics [2, 92, 96], and psychology research suggests that mentally processing the topical hierarchy of text is fundamental to the reading process [41].

286.123719732

====================
Finally, Precog explains why segments were predicted as low quality by selecting the feedback that is most relevant to changing the segment into a high quality prediction.

281.79839226

====================
Based on this library, we develop document-level and segment-level prediction models.

281.411904699

====================
In the rest of this paper, we use the term document to refer to the value of the acquired text attribute.

280.866820244

====================
We then map these feature combinations to explanation functions that are executed to generate the final set of feedback text (Section 5).

279.398785657

====================
For instance, the following defines the function for Off-Topic text:  7.1  Precog for Hard Constraints  Although it is intuitively obvious that form feedback and custom interfaces should improve quality, we quantify the amount using the example from Section 2.

274.200753452

====================
]P(d),pi 6=0  Putting this together, we can define the responsibility score Sed of a feature explanation function (FEF) e as the average of its bound features; where Fei [?]

269.355028629

====================
Quality control for crowdsourcing has been extensively studied [54] and can be modeled in two phases.

268.870315924

====================
For Host Profiles Precog improved the overall measure by nearly 7.1x over the baseline (0.65 vs. 0.07 increase), and a 1.7x improvement over the next-best Doc+TCruise condition.

267.065776567

====================
]Rn  P(d) =  [  Rather than examining all possible perturbations, our heuristic to compute Sfdi restricts the set of perturbations with respect to the decision paths in the trees that increase d's utility.

267.004530883

====================
Unfortunately, this procedure is not effective for non-continuous or low cardinality features such as one-hot encoded features (e.g., each word is represented as a separate binary feature) common in text analysis.

261.99579936

====================
However, it leaves it up to the user to infer specific improvements to make.

259.10227418

====================
The final FEATURE table review_feats is used in the later sections to represent the features extracted from the value of the primary key (review).

258.212168984

====================
By default we use this library for learning quality measures from a corpus.

254.326607511

====================
Third, it's unclear how to automatically generate the appropriate feedback text to show the user.

254.102282256

====================
We tested this hypothesis by running an experiment, using an existing corpus of Amazon reviews [61].

251.870971432

====================
Our approach is inspired by existing feedback systems--model features act as signals to identify text characteristics that the worker should change.

251.631873735

====================
These include guidelines and constraints on form elements [36, 69], as well as interface techniques such as double entry [20] commonly used for picking passwords.

251.537241269

====================
Segment-Predict-Explain: Based on these observations, Precog automatically identifies low-quality portions of a document, and generates feedback to help improve the identified issues.

250.744514393

====================
For instance, we might replace the rating domain constraint with five stars similar to Yelp and other social websites.

250.26723037

====================
For example, the following specify the star interface for rating and the autocomplete interface for product: CREATE INTERFACE ON reviews(rating) USING "stars" FROM "interfaces.js" AND explanation_function; CREATE INTERFACE ON reviews(product_id) USING "autocomplete" FROM "interfaces.js" AND explanation_function;  It addition, custom interfaces can be used to provide feedback that goes beyond textual feedback (e.g., visualizing distributions of common numerical values), or that is at a finer granularity than for the entire attribute.

248.199754242

====================
In addition, rather than compute the impact for all possible perturbations, we only consider the minimal perturbation with respect to each path in the tree.

247.772624946

====================
](minp(d, qij ))  If two paths within a tree perturb the same set of features, we only consider the path with the maximal impact score.

247.574746195

====================
It does so by generating feedback or interface changes to help workers improve their data pre-submission.

245.724327882

====================
However, such systems would not recognize that the review can be most improved by simultaneously reducing the emotion in the text and including more product details that ultimately increase the length.

241.828352625

====================
SEGMENT-PREDICT-EXPLAIN  The challenge with directly developing Precog quality control for text is that the quality score and explanation function is difficult to express as a concrete function, and they must be customized for the application domain.

240.703690908

====================
Although they are interpretable for simple constraints such as domain violations, the language for the uniqueness violation requires database familiarity and may not be accessible to non-technical experts.

239.853913764

====================
Crowd-based feedback is effective, but can take 20 minutes to generate feedback [52] and are essentially posthoc because they create new crowd tasks to refine previously submitted ones.

233.750044485

====================
We evaluate Precog for product_id (foreign key constraint) and rating (domain constraint) from the reviews table.

233.456677185

====================
We assume that the interface is a javascript function (say, as an AngularJS [19] or ReactJS [26] component); the constructor takes as input a Precog-provided getFeedback method that retrieves feedback from the Precog server.

231.267278822

====================
Finally, when the user hovers over a highlighted segment, more targeted feedback helps explain why it was identified as low quality and how it could be improved.

230.854527399

====================
To summarize, each participant was randomly assigned to one of four conditions: Doc+Krause, Seg+Krause, Doc+TCruise and Precog (Seg+TCruise ).

229.332922363

====================
The classic approach is to use simple, interpretable models [13, 53, 88] or to learn an interpretable model using the training  https://www.reddit.com/r/askscience/ We define > 1 net up-votes as helpful and <= 1 as unhelpful.

228.006743976

====================
Each facet defines high quality at a different threshold; product reviews and host profiles are shown as the top and bottom rows, respectively.

226.364349116

====================
In general, we must account for the amount that a feature must be perturbed, and the number of other features that must also be perturbed, in order to improve the classification.

224.816240357

====================
Figure 10 plots the mean change and 95% boostrap confidence interval for the four rubric scores.

222.767200716

====================
Section 3 describes the Segment-Predict-Explain pattern that helps developers easily customize interface for text attributes.

221.899450495

====================
Each measure is rated on a scale from 1 (Strongly Disagree) to 7 (Strongly Agree) based on coder agreement with a set of statements mapped to each criterion (i.e., "This person will stick to his/her word, and be there when I arrive instead of standing me up" for integrity).

215.548751692

====================
The primary features that we do not include are those that rely on application metadata such as the worker's history or location, which may be predictive of quality but not related to the writing content, and cannot be mapped to actionable writing feedback.

215.270745271

====================
Feature Library for Text Quality  PREDICT  Precog takes as input a training corpus of documents and document-level quality labels, and trains two models-- document-level and segment-level prediction models--in order to provide document-level and fine-grained segment-level feedback.

212.107955461

====================
Thus, we decompose the text into segments, and for each low-quality segment predicted by the model, we generate segment-specific feedback.

209.779336001

====================
We address the former challenge using a data-driven approach that learns a quality measure from data that has already been acquired.

209.11730468

====================
This can be achieved by dynamically identifying these constraint violations and providing feedback to the user.

203.725955469

====================
2 +15 However, there can be an infinite number of perturbations that all improve the utility--which should be selected?

203.198322944

====================
The left column shows the feedback interface generated by default.

202.474134593

====================
Figure 2 augments this workflow with pre-hoc quality control.

199.598132786

====================
While such approaches are often supervised in nature, requiring a manual topic ontology [57, 62], we use LDA [8] because it is unsupervised and can be quickly trained on any corpus without any cost to the developer.

194.060094003

====================
behind-the-enemy-lines.com/2011/04/want-to-improvesales-fix-grammar-and.html, 2016.

192.040623887

====================
Furthermore, in some settings where collecting more data is not an option (e.g., less popular products may not have enough users that are willing to, or equipped to, write reviews), it will be more important to apply pre-hoc quality control.

192.02635978

====================
Although developers can easily implement their own FEFs, Precog is pre-populated with 5 FEFs that work across the two application domains used for evaluation.

191.375163801

====================
* The design and implementation of Precog, which supports pre-hoc quality control for constraints over simple data types and quality measures over text and open-ended attributes.

190.342455905

====================
The general approach is to survey quality assessment research in a domain to borrow useful features and explanations.

186.150609206

====================
Document Labels for Segments: Despite generating topically coherent segments, we lack quality labels for training the predictive model at the segment level.

185.619381868

====================
The offline components (blue arrows) take as input a corpus of training data in the form of user generated text documents and their labels--for instance, Amazon product reviews may be labeled by the ratio of "helpful" and "unhelpful" votes.

184.139693275

====================
Further, review hierarchies were proposed for hierarchical crowdsourced quality control using expert crowds [39].

184.118024091

====================
The Journal of Forensic Psychiatry & Psychology, pages 1-21, 2017.

183.162811566

====================
INTRODUCTION Figure 2: Text acquisition with pre-hoc (beige background) and post-hoc quality control.

182.295060012

====================
al describe the meaning of the three Airbnb criteria in [57]: Ability "refers to the host's domain specific skills or competence."

180.799166903

====================
Thus, we wrote a friendliness explanation function that suggested writing more friendly and inclusive prose, and bound it to the relevant LIWC features (social, inclusive, etc).

180.700733809

====================
By default, Precog provides optimizations for constraints over numerical and categorical data types, and can be extended with custom optimizations.

176.950302419

====================
The key insight is to take advantage of the structure of the random forest model to constrain the types of perturbations and feature subsets to consider.

175.36962554

====================
Crowddb: answering queries with crowdsourcing.

175.302285199

====================
We then use explanation functions to transform the most responsible features into prescriptive feedback for the user.

175.060026376

====================
In addition to boolean constraints such as domain, foreign key, and uniqueness constraints, Precog also supports quality scores.

174.169517501

====================
The Model Generator then trains two classification models to predict the quality of a user's overall text submission as well as its constituent segments; these are cached in the Model Store.

173.022602697

====================
We then select the maximal  No feedback needed if data point already has high utility.

172.685162738

====================
Peerstudio: Rapid peer feedback emphasizes revision and improves performance.

172.407801031

====================
The online components (green arrows) send the contents of a text input widget, along with an optional corpus name, to the webserver.

170.930388844

====================
]Q (d) i i  The space of solutions for Problem 1 relies on enumerating all possible elements in the power set of the feature set F, which is exponential in size: 2|F | .

170.044081547

====================
The final submission was considered the post-feedback submission, and the initial submission upon pressing the I'm Done Writing was the pre-feedback submission.

167.378880253

====================
The key challenge is that training data only contains quality labels for entire documents (e.g., helpfulness for the full review), and it is unclear how to leverage them for training a segment-level model.

166.905618923

====================
For the reviews and profiles experiments, Precog acquires >= 2x and >= 2.6x more high quality documents than the baseline for thresholds of 5.5 and 6, respectively.

166.255411481

====================
At this point, users could click the Recompute Text Feedback button (median 1 click/participant), or press Submit to submit and finish the task.

165.623156166

====================
The review rubric asks coders to scores reviews on helpfulness to laptop shoppers, and the host profile rubric asks coders to score profiles based on trustworthiness to potential tenants.

165.148133957

====================
(e.g., readability, informativeness, etc), and implement a representative and extensible library of 47 text quality features.

164.896021565

====================
This means that for n features there are 2n possible sets of (maximal influence) perturbations to naively explore.

164.776273994

====================
We created a simple Mechanical Turk task that asked workers to submit the product model of their cell phone along with a 1 to 5 rating for the phone's quality; each 9  worker was paid $0.05 to complete the task.

164.035597682

====================
1 Similar to precogs in Minority Report [22], who identify and help "resolve" low-quality human action in the future, Precog identifies and helps resolve low-quality data before it is submitted in the future.

163.220199948

====================
Precog is able to adopt to the domains' different quality measures (helpfulness vs trustworthiness) with small configuration changes.

163.132517856

====================
For instance, multi-paragraph text attributes such as product reviews, forum comments, or rental descriptions are particularly challenging for several reasons.

162.92098581

====================
However, it still leaves it up to the user to infer specific improvements to make.

159.160333718

====================
Each worker was randomly assigned to one of three conditions, one for each of the interfaces shown in Figure 7.

158.383624652

====================
We trained workers on a separate sample of segments, along with explanations of why each segment was helpful or unhelpful.

158.160300827

====================
[6] M. S. Bernstein, G. Little, R. C. Miller, B. Hartmann, M. S. Ackerman, D. R. Karger, D. Crowell, and K. Panovich.

154.83271804

====================
Similarly, auto-complete may be used to provide feedback about existing categories in order to avoid duplicates when collecting categorical text [28, 71] (e.g., ice cream flavors, presidents).

154.692214085

====================
Participants were randomly assigned to one condition group; with (21,26,22,23) participants in conditions (1,2,3,4), respectively.

153.822783202

====================
We compared a segment binary classifier trained under this assumption with human evaluation.

153.758578688

====================
A closely related work from the database community is Usher [15], which have similar goals to improve data collection quality.

152.436647879

====================
The basic idea is to push data-quality constraints down to the data collection interface rather than validate them after data acquisition.

151.613836906

====================
Feature Explanation Functions  Section 2 introduced explanation functions that can take as input features in a FEATURE table whose primary key references the desired text attribute.

150.673296259

====================
provement and Subjective Coder Improvement Scores as the dependent variables, and TCruise and segmentation as the independent variables.

149.955909262

====================
reduces the developer's efforts by allowing them to express the quality score in terms of model features by defining a FEATURE table, and to define explanation functions over features of the text attribute.

148.931264272

====================
Automated approaches such as auto-graders primarily focus on predicting quality rather than generating feedback [4, 25, 58, 89]; others are limited to syntactic analysis [27, 35, 63], or generate overly simple writing feedback [7, 10, 49].

146.618524656

====================
Our technical contribution is a pre-hoc feedback system for multi-paragraph text.

145.496781872

====================
This groundwork reduces the task of applying Precog to new domains.

143.088288989

====================
We did not require new features for product reviews; we simply label reviews with >= 60% helpful votes as high quality and low otherwise.

140.614816731

====================
i  Picking FEFs: Once the feature scores have been computed, identifying the top-k FEFs is straightforward, and we compute each FEF's average impact score using a series of fast matrix operations.

139.733675694

====================
4.2  Document-level Prediction  to manually label the generated segments, but this will be very costly and time-consuming.

139.175132355

====================
Indirect Quality Mechanisms: Indirect methods such as community standards and guidelines [2, 5, 70] help clarify quality standards, while up-votes and ratings provide social incentives [11, 66].

138.409116246

====================
Participants writing Airbnb profiles were asked to "pretend that [they] are interesting in being a host on Airbnb" and to "write an Airbnb profile for [themselves]".

138.253832304

====================
Rather than define a concrete quality measure, Precog automatically learns the quality measure from a training corpus that contains documents along with their quality labels (for the entire document, not each segment).

137.576813147

====================
The impact function I() is identical, however it takes a path qij as input and internally computes the minimum perturbation minp(d, qij ).

136.442122756

====================
Figure 11: Subjective agreement to: "The post-feedback revisions improved on the pre-feedback review."

135.485696339

====================
Though Precog demonstrates the feasibility of such automated interfaces, it also reveals several areas of improvement.

134.852239677

====================
We identify five main categories across the existing literature (Table 1).

134.627099936

====================
Finally, the Similarity category reflects how many quality prediction approaches compare the input document to a gold-standard of text [47, 50].

133.739596568

====================
[36] R. M. Groves, F. J. Fowler Jr, M. P. Couper, J. M. Lepkowski, E. Singer, and R. Tourangeau.

133.701234706

====================
The resulting model (85% accuracy, balanced test set) was competitive with existing work [31].

132.935094646

====================
The second states that a review is written for a given product in the products table, and contains a numerical rating as well as the text of the review.

132.66353137

====================
1  In fact, instances of pre-hoc quality control are already commonly used in practice, both in the survey design literature [36] and as form design throughout the Internet.

132.487147006

====================
The main idea is to scan each tree in the random forest and compute responsibility scores local to the tree.

131.373375999

====================
For quantitative attributes, a common dataquality constraint is to ensure values are not out of bounds (e.g., human age should be above 0).

130.076604869

====================
3  function is used for domain constraints on reviews.rating and users.age.

129.837495864

====================
However this work either focuses on a particular application [91], or not intended to support custom interfaces [72].

129.20384384

====================
Fminp(d,q) = Fminp(d,q0 ) }  Problem 1 (Prescriptive Explanation).

127.324577893

====================
Normalization: We find that features closer to the root will happen to occur in more feature sets and have artificially higher scores, thus we need to adjust feature impact scores to reduce bias.

126.405460216

====================
For this reason, we first define the maximum influence perturbation set PF of a given subset of features F [?]

126.364626611

====================
These naturally map to 4 of our feature categories, so we wrote explanation functions for each and bound them to the features in the corresponding category.

124.774230236

====================
Our model performs competitively with prior work [32].

124.765685931

====================
A similar approach is applicable for regression models as well, where increasing the continuous prediction assigns the perturbation more responsibility.

123.98067045

====================
Survey Design and Optimization: The survey design literature has studied ways to re-ordering, and designing survey forms in order to reduce data entry errors.

123.584637501

====================
Figure 8 plots the number of high quality tuples that were collected as a function of the number of completed tasks; we define a tuple as high quality if no constraints were violated.

123.334683499

====================
impact paths for each tree; for each path, we add the responsibility score of all features perturbed in its minimum perturbation p. The final scores are used to select from the library of explanation functions.

121.952386469

====================
In order to generate targeted feedback, Precog automatically identifies topically coherent portions and segments the document in order to analyze each segment individually.

121.573621679

====================
By tackling low-quality data pre-acquisition, it can reduce or eliminate the need for post-hoc quality control.

120.646753201

====================
* A data-driven approach to estimate quality for text attributes, including a categorization and implementation of 47 text quality features from a survey of the literature.

120.499601195

====================
Consider a single tree in a random forest, consisting of decisions on two features, len and emotion.

119.933773721

====================
For instance, Amazon product reviews and users may be modeled using the following crowdbased DDL statements.

119.286010417

====================
The Segment-PredictExplain component has a beige background: Blue arrows depict the offline training and storage process and Green arrows depict the online execution flow when a user submits.

118.605152745

====================
]E  Sfdi =  The TCruise Heuristic Solution  X  X  I(d, qij ) if U (vij ) > U (M (d))  Ti [?

117.815531833

====================
Note that the same explanation 2  Note that the developer may express a CHECK constraint and the database can generate an (indecipherable) error message.

117.167456051

====================
To contrast, we focus on using explicit constraints and ambiguous quality measures (for text) and provide explicit DDL statements to push them to the input interface.

115.51353159

====================
For hard constraints (Purple), user inputs are sent to the  4  jargon usage, text similarity measures) that are trained to fit each developer's unique corpus.

114.434744182

====================
In this example, there are two ways to perturb the feature vector: by reducing the emotion feature by at least 20, or by increasing the length by at least 10 and reducing the emotion by at least 15.

112.723737685

====================
Finally, the most sophisticated may change the input element itself in order to constrain or fully customize the feedback (right column).

111.240124972

====================
For instance, len FEATURE len_extracton defines the feature returned by the user-defined function len_extracton.

109.995130924

====================
Once a library of features are given, the document-level prediction turns to be a typical classification problem.

109.22471814

====================
For instance, Amazon already has a corpus of high and low-quality reviews, and similarly for other applications.

108.486162844

====================
In practice, an FEF takes as input a list of features, as well as the text document and the full feature vector, and returns feedback text.

106.992453785

====================
The core challenges are to (1) identify a proxy for text quality that is consistent with the downstream application's needs, and (2) to generate effective feedback text.

106.440069877

====================
Rmxn represent the features bound to each of the m FEFs, where Aji = 1 if feature fi is bound to FEF ej , ~ e otherwise 0.

105.476250779

====================
If the features topics, featureCnt, and textLen have high responsibility, then it will be called to recommend new product features that the worker should mention in the review; the recommendations are dynamically selected based on the text's topic distribution (topics) and the number of product features detected (featureCnt < 10):  Setup: Let d [?]

105.367535367

====================
Participants were told that upon submitting their writing, they may receive feedback and could optionally revise.

104.851525915

====================
Purple arrows show the feedback process for hard constraints.

104.655463909

====================
TopicTiling outperformed other topic segmenters [44, 65] in terms of their WindowDiff score [74] as compared to a hand-segmented test corpus of 40 documents.

104.15432025

====================
71.3% had written a prior product review; all had read a product review in the past.

103.318751247

====================
Subjectivity is the extent that the review is fair and balanced but with enough helpful opinions for the buyer to make an informed decision: 1 means the review is an angry rant or lacks any opinions while 7 means it is a fair and balanced opinion.

102.508388802

====================
Although these user defined functions are powerful enough to support arbitrary analysis of an attribute value, such an approach is difficult to compose and extend, and the feedback is still limited to the entire attribute value.

101.332711197

====================
Further, the set of maximum influence perturbations is the union of PF for all feature sets:  minp(d, qij ) = arg min |p|2 s.t.

100.995217003

====================
We observe that document quality is sufficiently correlated with segment quality, and a document's label can be used to label its segments as training data for a segment classifier.

96.9154618924

====================
Figure 4: Examples of three levels of Precog quality control for four classes of data integrity constraints.

96.6584900648

====================
Subjectivity assesses user bias using a variety of features ranging from sentiment analysis [32, 34, 56] to pronoun usage [73].

96.3766461184

====================
We thus assign each participant to one of four conditions.

96.3656974376

====================
Participants were randomly assigned to one condition group; all conditions had 21 subjects except the Precog condition which had 22.

96.0015570765

====================
The backend splits the review into coherent segments, identifies the low-quality segments, and generates document-level feedback.

95.9820922243

====================
The full set of coder statements is described at length in [57].

95.800289518

====================
Given the feature vector of a data point d, prediction model M , a set of FEFs E = {e1 , * * * , em }, return the top k FEFs whose responsibility is above a threshold t:  Fp = {fi [?]

94.8170904947

====================
Customized Feedback: Precog provides a DDL for developers to customize feedback.

94.6982449245

====================
[38] I. Guyon, J. Weston, S. Barnhill, and V. Vapnik.

92.3022935925

====================
Such latency difference is relatively small if we compare the end-to-end time of two systems since the majority of the time was spent on worker recruitment.

91.8056324227

====================
We assume that the domains of the features have been normalized between [0, 1].

90.4974623499

====================
When the topic within the window changes significantly, then TopicTiling creates a new segment.

89.6994401334

====================
Although there are numerous segmentation algorithms, we describe the rationale for the choice of using a topic-based segmentation algorithm.

89.2225789287

====================
Second, it is ill-defined and applicationdependent, thus difficult to specify as a constraint.

89.1698062248

====================
Journal of Language and Social Psychology, 35(4):435-445, 2016.

89.0179086971

====================
support.google.com/docs/answer/57859, 2016.

88.8589394638

====================
Estimating the helpfulness and economic impact of product reviews: Mining text and reviewer characteristics.

88.5463107357

====================
Although there might be a number of segments mislabeled, the model can tolerate their impact well and achieve good performance.

88.3242057312

====================
Dij ), and the output of the random forest M (d) = arg maxv |{1|vij = v}| is the majority vote of its trees.

88.1927925911

====================
For instance, consider a document that contains a single segment--the segment may be high quality but the overall document is too short and is missing text for other topics.

87.9243427169

====================
We define Qi as the set of maximal impact paths of a tree Ti , with at most one path for a given subset of features.

87.6593090822

====================
Their work identified a subset of the Linguistic Inquiry and Word Count (LIWC) features [73] and other features as useful for measuring trustworthiness.

87.2943932333

====================
We used a qualification task to ensure participants had ever owned a laptop.

87.1627057971

====================
They pre-compute the "typical" values of each feature in the high quality corpus, then identify the "atypical" outliers in the test data's feature vector (e.g., a feature whose value is 1.5 standard deviations from the mean).

86.2948032723

====================
Precog uses existing techniques to generate forms for crowd workers to fill out, and the form contents are inserted as new records into the corresponding crowd table.

86.1152433395

====================
In contrast to typical integrity constraints, which will reject an inserted record that violates the constraint, Precog seeks to maximize its value.

84.7756512714

====================
For product reviews, Informativity is the extent that the review provides detailed information about the product, where 7 means that the review elaborates on all or almost all of the specifications of a product while 1 means that it states an opinion but fails to provide factual details (e.g., laptop specifications).

84.5429090437

====================
Usher analyzes an existing corpus of collected data to dynamically learn soft constraints on data values, and focuses on input placement, re-asking, and some interface enhancements.

84.2084393767

====================
For each scanned path q, we compute the change in the utility function, discount its value by the minimum perturbation p as well as the path's confidence.

84.0662947317

====================
For each feature fi , we compute the responsibility for each low quality text, and aggregate their values to compute the sample mean ufi and standard deviation sfi .

83.6981222252

====================
Let minp(d, qij ) return the minimum perturbation p (based on its L2 norm) such that d matches path qij .

83.5408951863

====================
4.3  Segment-level Prediction  There are two challenges in training a segment-level prediction model.

83.2198968204

====================
Overall Quality is the holistic helpfulness of the review for prospective buyers.

82.7391093894

====================
For the foreign key constraint, we populated a products table with all cell phone product models from the Amazon product corpus and a comprehensive list of phone models [93].

81.8731308973

====================
4.1  Existing automated writing feedback tools primarily focus on syntactic, simple errors [27, 35, 63].

80.8332520863

====================
On violations, the feedback generator creates custom feedback (if specified in a DDL statement) and the default or customized interface displays the feedback.

80.6431827954

====================
As constraints become more complex, there is a need for customized messages.

80.3414825081

====================
An example will be shown in Section 5.2.

80.2335331967

====================
Each measure is the average of the ratings from two coders--if they differed by >= 3, a third expert coder was used as the tie breaker and decided the final value.

79.914458734

====================
The default simply renders feedback generated from database constraint violations on tuple insertion (left column).

79.845091568

====================
Argonaut: macrotask crowdsourcing for complex data processing.

79.5087425191

====================
Other explanation functions (Topic, Informativeness) suggested specific content for the user to write about, mined from high-quality documents from each corpus (i.e., topics, jargon).

78.952223844

====================
Ability "refers to the host's domain specific skills or competence."

78.5618443979

====================
Consider a review consisting of a long, angry diatribe about customer service.

78.4602767713

====================
The following snippet sketches the Not Enough Detail function in our evaluation.

75.894502902

====================
It uses a sliding window to compute the LDA [8] topic distribution within each window and create a new segment when the distribution changes beyond a threshold.

75.7493480953

====================
In Eighth International AAAI Conference on Weblogs and Social Media, 2014.

75.6154111107

====================
The green path (p1 ) must at least reduce emotion by -20; the blue path (p2 ) must at least increase length by 10 and at least reduce emotion by -15.  input text.

74.3660472662

====================
One approach is to simply highlight the low-quality segment and provide generic/static feedback.

73.8302444024

====================
Given d and predicted utility U (M (d)), we retrieve and scan the paths with higher utility.

73.8241231218

====================
Some constraints, such as domain constraints, are registered as syntax errors.

73.7435709489

====================
Benevolence "refers to the host's domain specific skills or competence."

73.5435940345

====================
An overview of current research on automated essay grading.

73.3583964951

====================
Figure 4 summarizes Precog into three levels based on the amount of customization needed by the developer.

73.2740395788

====================
Truth discovery and crowdsourcing aggregation: A unified perspective.

72.4165995683

====================
TKDE, 2016.

72.1695106139

====================
quora.com/What-percentageof-questions-on-Quora-have-no-answers, 2016.

72.1364725974

====================
However, we may use a slider if for larger cardinalities.

70.7247802426

====================
For instance, the bottom row of Figure 4 illustrates fine-grained feedback in the form of both highlighted text and text feedback for individual segments that the user has written for reviews.review.

70.7236164432

====================
Figure 7 depicts the three interfaces that are created--naive with no Precog, customized feedback, and customized interface optimizations.

70.4675296297

====================
Feedback and interface customization acquire 1.7x and 1.9x more high quality tuples than no Precog optimization.

70.4127458391

====================
[9] R. Boim, O. Greenshpan, T. Milo, S. Novgorodov, N. Polyzotis, and W. C. Tan.

70.0501717029

====================
](p)  C can be chosen based on the model--for a random forest, we define C as the percentage of trees that vote for the majority label.

69.8998689637

====================
However, Precog is more effective when the threshold increases.

69.7819617992

====================
Custom Interface: Fully customizing the interface component is useful in order to directly prevent users from submitting invalid attribute values.

68.7797957185

====================
Designing novel review ranking systems: predicting the usefulness and impact of reviews.

68.4010166459

====================
The basic idea is to push data-quality constraints down to the data collection interface and improve data quality before acquisition.

68.3256828496

====================
[30] H. Garcia-Molina, M. Joglekar, A. Marcus, A. G. Parameswaran, and V. Verroios.

68.3102779859

====================
Reading through their table of features, we also found that writing style and friendliness features were common.

68.2842255831

====================
The Segment-Predict-Explain component consists of offline and online components.

68.27011547

====================
In the long term, we envision Precog as an example of automatically applying pre-hoc quality control (e.g., writing feedback) based on downstream application needs (e.g., quality reviews).

68.2476495564

====================
An alternative is to use existing model explanation algorithms [76] to describe the prediction.

68.0462449306

====================
This can be directly computed by examining the decision points along the path.

67.8178041701

====================
The Feedback Generator then constructs feedback explanations for the low quality text, which are returned and displayed in the widget.

67.7758814783

====================
Precog uses the models in the Model Store to identify whether the entire document and/or segments generated by the Segmenter are low quality.

67.7222061804

====================
Each defines the three main measures, and provides examples that contribute positively and negatively to each criteria.

67.6377486409

====================
Assuming that C() = 1, p1 's impact on the input document is I(d, p1 ) = 1-0 x1= 20 0.05, whereas p2 's impact is I(d, p2 ) = 1021-0 x 1 = 0.055.

67.0975712758

====================
For instance, qualreview seeks to maximize the quality score as defined by qual_udf.

66.9009675244

====================
The first states that user information is collected from the crowd (of Amazon users) and that the username must be unique.

65.9414554875

====================
Thus, it is clear that the emotion should be assigned a greater responsibility because there are more branches for which changing its value will contribute to a better classification.

65.7729245186

====================
Feedback and interface customization acquire 1.7x and 1.9x more valid records than no Precog optimization.

65.1325662306

====================
Intuitively, the FEF should be executed if its list of features F can take "highly responsibility" for improving the quality score.

65.1309229388

====================
Precog denotes the segment-level TCruise-based system.

64.9844853847

====================
, qik ; each path qij matches a subset of the training dataset Dij [?]

64.2319981076

====================
[28] M. J. Franklin, D. Kossmann, T. Kraska, S. Ramesh, and R. Xin.

63.2298056019

====================
Similarly, Airbnb profiles took an average of 10.2 minutes to complete without Precog and 15.3 minutes to complete with Precog.

62.05440295

====================
Developers commonly implement explanation functions to generate more user-friendly feedback (middle column).

61.9765293467

====================
[71] A. G. Parameswaran, H. Park, H. Garcia-Molina, N. Polyzotis, and J. Widom.

61.9612140239

====================
In these examples, we simply define a python function.

60.938793246

====================
The confidence C(d) is the fraction of samples in Dij whose labels yk match the path's prediction vij .

60.8156675797

====================
For instance, in a binary classification problem U may return 1 if the input is "high quality" and 0 otherwise; in a regression model, U may be the identity function.

60.3986190709

====================
Crowdsourced enumeration queries.

60.3073342205

====================
Crowdsourced data management: A survey.

60.1009058693

====================
Figure 7: Worker interfaces to evaluate no optimization, custom feedback Precog, and custom interface Precog for hard constraints.

59.5202099119

====================
Precog uses the feature library to transform the input text into a feature vector of (len=10, emotion=30), and is thus classified as low quality.

59.3969267076

====================
All trustworthiness factors except friendliness directly corresponded to existing explanation functions.

59.1777082759

====================
Thus, the output of Ti (d) is the vote vij of the path that matches d (e.g., d [?]

58.3096762807

====================
Precog is agnostic to the specific segmentation algorithm, and developers can use their own.

57.9567219493

====================
We used a post-study survey to collect demographic information as well as their subjective experience.

57.9205975769

====================
We describe this in Section 4.

57.7874482944

====================
Texttiling: Segmenting text into multi-paragraph subtopic passages.

57.7029751688

====================
We plot CDF curves for the number of high quality documents as the task budget increases.

57.5913877154

====================
What in the hay is a zappos premier reviewer?

56.9958410306

====================
Crowdforge: Crowdsourcing complex work.

56.8886343228

====================
Consider the perturbations p1 , p2 in Figure 6.

56.7363551822

====================
5.2  Figure 6: Assigning responsibility to perturbations.

56.6845571246

====================
First, the quality measure is continuous (there is no "perfect document") and thus hard to identify a "violation".

56.3421749157

====================
The second challenge is to determine how the available document-level labels can be used for training segment-level quality.

56.1495996216

====================
We relaxed the foreign key constraint by ignoring case sensitivity of the product names.

55.892619603

====================
User-facing Interface: The custom interface column for the quality score in Figure 4 depicts the Precog interface in action.

55.7587735215

====================
A method to automatically choose suggestions to improve perceived quality of peer reviews based on linguistic features.

55.0854637739

====================
Moreover, there have been many successful attempts to use topic distributions to predict quality [55, 57, 57].

54.8921019641

====================
[57] X. Ma, J. T. Hancock, K. L. Mingjie, and M. Naaman.

54.7393600914

====================
http://www.zappos.com/premier-reviewers, 2016.

54.4853586835

====================
Dij |yk = vij }| |Dij |  Qi (d) = {q [?]

54.4552717779

====================
Rn be a data point (text document or segment) represented as a feature vector, where di corresponds to the value of fi .

54.1244230581

====================
These methods focus more on finding good contributors and lack content-specific feedback (e.g., discuss camera quality for a phone).

53.6901151567

====================
Clearly, document quality assessment is a well-studied area.

53.5291116378

====================
The user writes a product review in the textbox; the content is sent to the Precog backend via getFeedback().

53.381515744

====================
Survey on web spam detection: principles and algorithms.

52.8842123589

====================
Developers can bind the interface to an attribute using a CREATE INTERFACE statement.

52.8038126489

====================
[47] S.-M. Kim, P. Pantel, T. Chklovski, and M. Pennacchiotti.

52.6128050321

====================
Challenges in data crowdsourcing.

52.4752963611

====================
Overall Quality is the holistic trustworthiness of the host for prospective tenants.

51.1482387455

====================
[13] R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm, and N. Elhadad.

50.1415907167

====================
3 Note that databases automatically generate names for almost all integrity constraints.

49.529004784

====================
[69] K. Norman, S. Lee, P. Moore, G. Murry, W. Rivadeneira, B. Smith, and P. Verdines.

49.1904830098

====================
Incentive mechanisms such as badges, scores [21, 33], status [97], or even money [42, 46] have also been used to keep good contributors.

48.9562690648

====================
[53] B. Letham, C. Rudin, T. H. McCormick, D. Madigan, et al.

48.6354838762

====================
The average task completion time was 11 minutes, and payment was $2.5 (~ $13.6/hr).

48.4577389437

====================
Human-powered sorts and joins.

48.4471749213

====================
[39] D. Haas, J. Ansel, L. Gu, and A. Marcus.

47.8833928481

====================
Recommending the world's knowledge: Application of recommender systems at quora.

47.6514164555

====================
Moreover, none focus on multi-paragraph text attributes such as product reviews or forum comments.

47.6452734074

====================
[44] J. C. T. Ji-Wei Wu.

47.6110524048

====================
jury selection for decision making tasks on micro-blog services.

47.5336815971

====================
[11] A. Bosu, C. S. Corley, D. Heaton, D. Chatterji, J. C. Carver, and N. A. Kraft.

47.5229625819

====================
[43] P. G. Ipeirotis, F. Provost, and J. Wang.

47.4804637218

====================
Features are individually mapped to pre-written feedback text [7, 10, 49].

47.3606654651

====================
[81] V. S. Sheng, F. Provost, and P. G. Ipeirotis.

47.3544053536

====================
It will cause the impact function to converge to 0 as the perturbations become larger.

47.3337809627

====================
A tree Ti is composed of a set of k decision paths qi1 , .

47.0450777004

====================
]Fe  C(d) =  i  |Fe |  x C(d + minp(d, qij ))  |{dk [?]

47.0008147084

====================
In System Sciences (HICSS), 2011 44th Hawaii International Conference on, pages 1-10.

46.8141550571

====================
The average task completion time was 14 minutes, and payment was $2.5 (~ $10/hr).

46.4917454854

====================
D and its vote vij is the majority label in Dij .

46.4265653439

====================
Although the data cleaning literature has proposed ways to prescribe data cleaning operations [14], they are not applicable for text attributes.

46.3757696183

====================
Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission.

46.3604706

====================
[1] E. Agichtein, C. Castillo, D. Donato, A. Gionis, and G. Mishne.

46.0989992372

====================
The developer then binds an explanation fuction to the appropriate constraint.

45.9714534158

====================
Linguistic analysis of chat transcripts from child predator undercover sex stings.

45.9288954739

====================
An FEF e : R|F| - text maps the feature vector for a subset of features F [?]

45.6867594161

====================
[3] N. Archak, A. Ghose, and P. G. Ipeirotis.

45.3513770467

====================
REFERENCES  [25] N. Farra, S. Somasundaran, and J. Burstein.

45.2165711094

====================
For instance, F may be the text features described above, and a data point corresponds to the extracted text feature vector.

45.2130122541

====================
For these, Precog generates default names of the form <table>_<attribute>_<type>.

45.1839368525

====================
Since the quality score is not a boolean constraint, feedback is simply not generated for it2 .

45.1759153132

====================
Scoring persuasive essays using opinions and their targets.

44.6899543659

====================
: analyzing and predicting youtube comments and comment ratings.

44.5533118919

====================
[59] A. Marcus, E. Wu, D. Karger, S. Madden, and R. Miller.

44.5175369172

====================
[41] J. Hyona, R. F. Lorch Jr, and J. K. Kaakinen.

44.0581966164

====================
The change in these measures between pre and post-feedback suggests the utility of the feedback.

43.8393187593

====================
Deriving the pricing power of product features by mining consumer reviews.

43.7050716531

====================
Given a small test corpus of pre-segmented documents, Precog can benchmark the algorithms and recommend the one with the highest WindowDiff score.

43.3456919878

====================
For the sake of exposition, product_id is the textual name of the product.

43.1545315834

====================
[23] M. Drouin, R. L. Boyd, J. T. Hancock, and A. James.

43.068520031

====================
for product reviews, and "The post-feedback revisions are more trustworthy than the prefeedback profile."

43.0667648275

====================
[48] A. Kittur, B. Smus, S. Khamkar, and R. E. Kraut.

43.0035699626

====================
In SIGMOD, pages 445-456, 2014.

42.9677875618

====================
In Review of educational research, 1988.

42.8723529703

====================
Feedback systems are typically based on outlier detection [50].

42.5478286851

====================
iCrowd: an adaptive crowdsourcing framework.

42.281104659

====================
The average host profile took 6.8 minutes to complete without Precog, and 11.1 minutes with the additional feedback from Precog.

42.2606834411

====================
html?nodeId=201929730, 2016.

42.1057095624

====================
Readability is the extent that the review facilitates or obfuscates the writer's meaning.

42.0417923869

====================
[90] G. Wang, K. Gill, M. Mohanlal, H. Zheng, and B. Y. Zhao.

41.6865452769

====================
Individual differences in reading to summarize expository text: Evidence from eye fixation patterns.

41.600758912

====================
[32] A. Ghose and P. G. Ipeirotis.

41.5795608278

====================
Winning arguments: Interaction dynamics and persuasion strategies in good-faith online discussions.

41.4564351408

====================
[29] J. Gao, Q. Li, B. Zhao, W. Fan, and J. Han.

41.1569163928

====================
In Computational Linguistics.

41.126190547

====================
Recall the feedback in the custom Precog interface in Figure 4, it identifies that the segment is short on details and suggests new topics.

40.429865559

====================
Moreover, Precog also makes it easy for developers to add custom segmentation algorithms.

40.388060411

====================
[31] A. Ghose and P. G. Ipeirotis.

40.372611867

====================
In contrast, segment level feedback is needed in order to provide specific, actionable suggestions that may not be evident at the document level.

40.3532748807

====================
improving data quality and data mining using multiple, noisy labelers.

40.2872209404

====================
[87] B. Trushkowsky, T. Kraska, M. J. Franklin, and P. Sarkar.

40.1685056533

====================
[73] J. W. Pennebaker, R. L. Boyd, K. Jordan, and K. Blackburn.

39.8926764222

====================
F is the set of features bound to an FEF: P Sed =  fi [?

39.6974521586

====================
[60] D. M. Markowitz and J. T. Hancock.

39.0361053344

====================
In isolation, existing approaches may find that the length is large and suggest reducing it, and that the emotion is high and suggest reducing it.

39.0332772439

====================
The feedback literature suggests that precise, local feedback is effective [68].

39.0001430371

====================
Further, developers can easily extend the library with custom features.

38.7890300639

====================
The coders labeled documents in random order and did not have access to any other information about the documents.

38.5043919661

====================
[82] S. Siersdorfer, S. Chelaru, W. Nejdl, and J. San Pedro.

38.4958456199

====================
Due to a small number of explanation functions, study participants found that repeatedly using the system began to provide redundant feedback; simplifying the development of more explanation functions may help the system produce more nuanced feedback.

38.3961525553

====================
There are some works that apply pre-hoc quality control to improving crowd quality [72,87,91].

38.3779839642

====================
Unanswered quora.

38.3500625146

====================
Pn j Aji is the average i=1 impact score of all features mapped to the jth FEF.

38.1976282488

====================
Double data entry: what value, what price?

38.1854721459

====================
Fan, G. Li, B. C. Ooi, K. Tan, and J. Feng.

38.1711810425

====================
Ultimately, existing feedback and explanation approaches are descriptive of the prediction, rather than prescriptive of the changes that must be made.

38.1685496198

====================
The Segmenter first splits each document into segments.

37.8617108309

====================
An explicit feedback system for preposition errors based on wikipedia revisions.

37.4480200811

====================
In addition, we do not compare paths across trees.

37.3232924989

====================
[14] A. Chalamalla, I. F. Ilyas, M. Ouzzani, and P. Papotti.

37.2816977816

====================
[18] L. Connors, S. M. Mudambi, and D. Schuff.

36.8678753186

====================
pi 6= 0 if fi is perturbed, otherwise pi = 0.

36.8493140003

====================
Towards globally optimal crowdsourcing quality management: The uniform worker setting.

36.6609313845

====================
[91] S. E. Whang, J. McAuley, and H. Garcia-Molina.

36.4305881558

====================
Check spelling and grammar in office 2010 and later.

36.2160601258

====================
In many cases, such as text attributes, it is desirable to provide feedback for specific segments of the text value.

36.1826566132

====================
The institutionalization of youtube: From user-generated content to professionally generated content.

36.0307677109

====================
AngularJS web application development.

36.0260313789

====================
](p) = |p|2 , the L2 norm of the perturbation vector.

35.980264948

====================
[8] D. M. Blei, A. Y. Ng, and M. I. Jordan.

35.9415865516

====================
https://www.nfcworld.com/nfc-phones-list/, 2016.

35.9191094802

====================
[54] G. Li, J. Wang, Y. Zheng, and M. J. Franklin.

35.7000356576

====================
[65] H. Misra, F. Yvon, O. Cappe, and J. Jose.

35.6880578178

====================
Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model.

35.6863143507

====================
Automatically assessing review helpfulness.

35.3776261103

====================
Figure 9: # of documents where quality >= thresh, for varying thresholds; product reviews (top), host profiles (bottom).

34.8556730753

====================
[86] J. Tang, X. Hu, and H. Liu.

34.8402630897

====================
F as the set of perturbations that only perturbe features in F and have the maximal influence.

34.8010321974

====================
We also used document-quality labels to train the segment classifier.

34.6051995547

====================
Interacting with predictions: Visual inspection of black-box machine learning models.

34.5473185991

====================
It has been extensively studied in recent years [9,12,24,29,39,80,98].

34.5381048858

====================
[98] Y. Zheng, J. Wang, G. Li, R. Cheng, and J. Feng.

34.488992373

====================
https://www.amazon.com/gp/help/customer/display.

34.3749288172

====================
Further, their analyses are per-feature and don't account for multi-feature interactions.

34.2790415416

====================
[49] J. Krause, A. Perer, and K. Ng.

34.0713230628

====================
When the threshold is low, it is easy to acquire low-quality text and both approaches are the same.

33.8824119976

====================
The key insight is that the predictive model is robust to noisy labels.

33.8683362911

====================
Check spelling and grammar in google docs.

33.7186853067

====================
[80] A. D. Sarma, A. G. Parameswaran, and J. Widom.

33.700653077

====================
We now describe the prediction model we use for documentlevel prediction.

33.6465771184

====================
We describe how Precog automatically generates feedback text for low-quality text.

33.4253621547

====================
QASCA: A quality-aware task assignment system for crowdsourcing applications.

33.3403188556

====================
[85] C. Tan, V. Niculae, C. Danescu-Niculescu-Mizil, and L. Lee.

33.3356877709

====================
In ACM SIGKDD workshop on human computation, 2010.

32.9235194209

====================
database, which checks that the input satisfies the integrity constraints.

32.6090086262

====================
[21] S. Deterding, D. Dixon, R. Khaled, and L. Nacke.

32.5076025625

====================
Rn where ~si = Snormdi , and matrix A [?]

32.4685667291

====================
[70] O. Nov. What motivates wikipedians?

32.4423578682

====================
Note that the quality criteria differ across domains.

32.2697421135

====================
While the idea is easy to achieve for simple data types and constraints, it faces significant challenges for text documents.

32.2525580788

====================
en.wikipedia.org/wiki/Wikipedia: Policies_and_guidelines, 2016.

32.2066248391

====================
CrowdFill: collecting structured data from the crowd.

32.0160681924

====================
These approaches incur additional quality control costs and are complementary to Precog.

31.5344111479

====================
Quality management on amazon mechanical turk.

31.4504998967

====================
The experiment was run until 100 workers had participated in each condition.

31.4480280832

====================
62% had used AirBnb before.

31.4417331431

====================
We defer this to future work.

30.8614500412

====================
Precog can automatically control the generated feedback by reallocating responsibility.

30.3522955981

====================
Vader: A parsimonious rule-based model for sentiment analysis of social media text.

30.335213568

====================
Note that the baseline does not acquire any high quality reviews when thresh >= 6.

30.2687642255

====================
Overall, each explanation function was 3-20 lines of python code.

30.1015941934

====================
Inferring networks of substitutable and complementary products.

30.0357461914

====================
qij matches d + p p[?

29.9929712013

====================
How useful are your comments?

29.9418946334

====================
For instance, a review that consists of many ambiguous phrases like "I have never done anything crazy with it and it still works."

29.8597565304

====================
Automated essay scoring with R v. 2.0.

29.8415628986

====================
Descriptive and prescriptive data cleaning.

29.5863887863

====================
]Rn  PF (d) = arg max I(d, p) s.t.

29.1587443642

====================
The primary groups of features related to absence of detail and low topic diversity.

28.601492776

====================
A path is the sequence of decisions from the root of a tree to a leaf node.

28.5236446382

====================
Scale-driven automatic hint generation for coding style.

28.2966751925

====================
In KDD, 2015.

28.2696958491

====================
In Advances in neural information processing systems, pages 121-128, 2008.

28.1738270199

====================
Social influence and the diffusion of user-created content.

28.1509592308

====================
Supersparse linear integer models for optimized medical scoring systems.

28.0747348522

====================
In reality, there is often a long tail of topics without sufficient content for such approaches to be effective [61, 79].

28.0600980178

====================
a multi-method approach to determine the antecedents of online review helpfulness.

27.9893914651

====================
Figure 9 compares Precog against the baseline of not using Precog (naive review collection).

27.8430916106

====================
[22] P. K. Dick, S. Spielberg, T. Cruise, and S. Morton.

27.8287243958

====================
E * = topk Sed s.t.

27.7927653375

====================
In ETS Research Report Series.

27.7134578415

====================
Figure 8: # records satisfying both constraints vs budget.

27.6904116257

====================
[61] J. McAuley, R. Pandey, and J. Leskovec.

27.5892478965

====================
[76] M. T. Ribeiro, S. Singh, and C. Guestrin.

27.5777366573

====================
Packt Publishing Ltd, 2015.

27.5524969133

====================
[55] J. Liu, Y. Cao, C.-Y.

27.5363209684

====================
Social computing and user-generated content: a game-theoretic approach.

27.5356212551

====================
Controlled clinical trials, 19(1):15-24, 1998.

27.2437907853

====================
[16] A. I. Chittilappilly, L. Chen, and S. Amer-Yahia.

27.2148259468

====================
Fix reviews' grammar, improve sales.

27.135969996

====================
Survey methodology, volume 561.

27.1332940075

====================
is assigned 1 as it might require multiple readings to understand.

27.0245415038

====================
[94] M. N. Xiao Ma, Trishala Neeraj.

26.8030966302

====================
Gene selection for cancer classification using support vector machines.

26.7051218573

====================
[67] S. M. Mudambi and D. Schuff.

26.6773678731

====================
[83] R. Singh, S. Gulwani, and A. Solar-Lezama.

26.6497037327

====================
[5] E. Bakshy, B. Karrer, and L. A. Adamic.

26.359166292

====================
Compare me maybe: Crowd entity resolution interfaces.

26.3568894469

====================
From game design elements to gamefulness: defining gamification.

26.191314492

====================
[15] C. C. Chen and Y.-D. Tseng.

26.1755808569

====================
[89] S. Valenti, F. Neri, and R. Cucchiarelli.

26.1440422332

====================
[58] N. Madnani and A. Cahill.

25.8713989464

====================
Automated semantic grading of programs.

25.578609298

====================
7.2.1  Segment, Explain, or Both?

25.5296997692

====================
Automating hint generation with solution space path construction.

25.4519193316

====================
http://www.imdb.com/title/tt0181689/, 2002.

25.3748846945

====================
To do so, we draw a sample of text from the corpus that has been labeled as low quality.

25.3469425617

====================
An efficient linear text segmentation algorithm using hierarchical agglomerative clustering.

24.8411354833

====================
In NAACL, 2015.

24.7283200845

====================
Both are important because they address different text quality factors.

24.6766464034

====================
Linguistic obfuscation in fraudulent science.

24.5847446312

====================
In SIGMOD, 2015.

24.3601858972

====================
Wiley e-rater Online Library, 2004.

24.260540505

====================
https://textblob.readthedocs.io/en/dev, 2014.

24.2352042708

====================
Low-quality product review detection in opinion summarization.

24.2078167656

====================
, Tt } is composed of a set of trees.

23.9634246939

====================
PVLDB, 2015.

23.79146812

====================
Sections 4 and 5 surveyed work related to text quality prediction and writing feedback.

23.7845051247

====================
[52] C. E. Kulkarni, M. S. Bernstein, and S. R. Klemmer.

23.7764536597

====================
The development and psychometric properties of liwc2015.

23.7287343753

====================
These ideas can be viewed as instances of Precog.

23.6645343535

====================
Justification narratives for individual classifications.

23.3704894267

====================
Wisdom in the social crowd: an analysis of quora.

23.3109244948

====================
[42] P. Ipeirotis.

23.3107121665

====================
a study of customer reviews on amazon.com.

23.2786478546

====================
The paths go from the document's current low quality classification to a high quality classification.

23.2557743348

====================
In Learning Research and Development Center, 2007.

23.2195633288

====================
Packt Publ., 2013.

22.9090956911

====================
[4] Y. Attali and J. Burstein.

22.8920112304

====================
[64] B. L. Minqing Hu.

22.882300229

====================
[72] H. Park and J. Widom.

22.8624974295

====================
A critique and improvement of an evaluation metric for text segmentation.

22.8268158687

====================
Respondable: Personal ai assistant for writing better emails.

22.79572924

====================
Social influence bias: A randomized experiment.

22.7775887921

====================
Self-disclosure and perceived trustworthiness of airbnb host profiles.

22.6117224159

====================
List of nfc phones.

22.5925521889

====================
Textblob: Simplified text processing.

22.4927106423

====================
These can be integrated as feedback and interface customizations in Precog.

22.425216568

====================
Asking the right questions in crowd data sourcing.

22.1437275961

====================
Deco: declarative crowdsourcing.

21.9845703356

====================
The random forest model M = {T1 , .

21.5492220663

====================
10  7.2.2  Ma et.

21.4649486762

====================
A. Kulik and C.-L. C. Kulik.

21.3006618302

====================
Building reputation in stackoverflow: an empirical investigation.

21.2898544785

====================
]F  Based on these definitions, the total responsibility of a given feature fi [?]

21.258808777

====================
Is it the review or the reviewer?

21.1460811151

====================
[84] N. Spirin and J. Han.

20.9886391861

====================
No participant had used Precog before.

20.9861326333

====================
Precog only marginally increases latency of each worker.

20.9860357519

====================
Rn is a vector that modifies a data point.

20.9291373429

====================
[62] J. D. Mcauliffe and D. M. Blei.

20.7390036238

====================
Mining opinion features in customer reviews.

20.5809523993

====================
[78] K. Rivers and K. R. Koedinger.

20.5003381068

====================
In TKDE, 2011.

20.4922414865

====================
Existing feedback approaches are not directly applicable for Precog.

20.4737959359

====================
She, Y. Tong, and L. Chen.

20.4436567264

====================
A perturbation p [?]

20.3759051654

====================
[74] L. Pevzner and M. A. Hearst.

20.3176469507

====================
[19] P. B. Darwin and P. Kozlowski.

20.3167177294

====================
Topictiling: a text segmentation algorithm based on lda.

20.3125627569

====================
PVLDB, 2012.

20.2813222243

====================
[7] O. Biran and K. McKeown.

20.2454439369

====================
American Association for the Advancement of Science, 2013.

20.0845122185

====================
Are both Segment and Explain necessary in the SegmentPredict-Explain pattern?

20.0775003752

====================
81 completed the task.

20.0676310374

====================
Technical report, Stanford InfoLab, 2012.

20.0620762333

====================
Finding high-quality content in social media.

19.9523853891

====================
In Journal of Information Technology Education, 2003.

19.935484719

====================
[88] B. Ustun and C. Rudin.

19.7030916928

====================
": Explaining the predictions of any classifier.

19.6020557397

====================
Social recommender systems.

19.4909015761

====================
[40] M. A. Hearst.

19.4152611025

====================
[66] L. Muchnik, S. Aral, and S. J. Taylor.

19.1655154623

====================
[95] L. Yang and X. Amatriain.

19.1270006211

====================
In NAACL, 2014.

18.8072378802

====================
The document level feedback provides a global quality assessment.

18.6679934855

====================
Latent dirichlet allocation.

18.6440290743

====================
A survey of general-purpose crowdsourcing techniques.

18.6301926508

====================
A computational approach to perceived trustworthiness of airbnb host profiles.

18.6206828701

====================
In ACM SIGecom Exchanges.

18.6155896194

====================
[20] S. Day, P. Fayers, and D. Harvey.

18.4929064279

====================
yelp.com/guidelines, 2016.

18.2614806751

====================
We start with the feature library of 47 features and no explanation functions.

18.193012054

====================
Lin, Y. Huang, and M. Zhou.

18.1758497169

====================
In Machine learning.

17.8311306728

====================
The nature of feedback: Investigating how different types of feedback affect writing performance.

17.6190704318

====================
http://www.boomeranggmail.com/respondable/, 2016.

17.5230011522

====================
In VLDB, 1994.

17.4629603412

====================
In SIGKDD, 2016.

17.3863219022

====================
[26] A. Fedosejev.

17.2245023505

====================
preprint maxiao.info, 2017.

16.864580484

====================
Soylent: A word processor with a crowd inside.

16.8004294489

====================
How much work does it take to add rich feedback support for text in a new domain?

16.7840295528

====================
The interface was the same for all conditions--only the feedback content changed.

16.5851857253

====================
[96] Yelp.

16.4299166379

====================
A model M : Rn - N classifies a data point as M (d) [?]

16.4076684142

====================
Timing of feedback and verbal learning.

16.1816844752

====================
F is based on the responsibility of each perturbation that involves the feature.

16.0443271873

====================
Social recommendation: a review.

15.96399323

====================
foxtype.com/, 2016.

15.8689479438

====================
John Wiley & Sons, 2011.

15.8253049067

====================
INFORMS, 2011.

15.6431194381

====================
[17] R. R. Choudhury, H. Yin, and A.

15.6060881706

====================
[77] M. Riedl and C. Biemann.

15.5160449015

====================
Amazon: Community guidelines.

15.5072037565

====================
Online survey design guide, 2003.

15.3874389838

====================
In Recommender Systems Handbook.

15.360846208

====================
Technical report, MIT, 2012.

15.2494728605

====================
The Annals of Applied Statistics, 2015.

15.2101845451

====================
Write smarter emails.

15.1451071469

====================
In ICITS, 2014.

15.0761837958

====================
6  data near the test point [76].

15.044500548

====================
They also assume a large corpus that contains high quality content for every topic (e.g., product or question).

15.0332936604

====================
[97] Zappos.

14.9459544295

====================
American Psychological Association, 2002.

14.9214229078

====================
Sed > t e[?

14.767019782

====================
In SIGMOD, 2011.

14.7115487688

====================
Rm = A~s.

14.6298713275

====================
In KDD, 2012.

14.4535922506

====================
[34] C. H. E. Gilbert.

14.2098246421

====================
Editor guidelines.

14.1773507062

====================
Fast algorithm for mining association rules.

14.1541563499

====================
In MIS quarterly, 2010.

14.148774881

====================
Text segmentation: A topic modeling perspective.

14.0688255956

====================
N, and a utility function U : N - R maps a label to a utility value.

13.9984057395

====================
[50] M. Krause.

13.9917595965

====================
For such cases, improving quality during user input process may be more effective.

13.9706145068

====================
VLDB, 2015.

13.8488289009

====================
In MSR, 2013.

13.8156968168

====================
[75] R. S. Rakesh Agrawal.

13.6616061073

====================
Minority report.

13.5831803996

====================
[68] M. Nelson and C. Schunn.

13.4190840915

====================
Content guidelines.

13.4082176187

====================
In HCI, 2016.

13.3182699805

====================
In ITS, 2016.

13.311104671

====================
MIT Press, 1997.

13.2530772796

====================
In KDD, 2008.

13.2466432898

====================
In UIST, 2011.

13.2240120543

====================
can be similarly defined in multiple ways.

13.0854329505

====================
In SIGMOD, 2014.

13.0702256923

====================
In WWW, 2013.

12.7861447331

====================
In HCOMP, 2015.

12.5757958916

====================
Precog is more effective than no Precog when the desired quality is high.

12.5042155048

====================
In CSCW, 2017.

12.4280704956

====================
rfk/pyenchant, Jan 2011.

12.4073463887

====================
[27] FoxType.

12.3004878623

====================
In UIST, 2010.

12.1742754388

====================
, dm } be the training dataset and Y = {y1 , .

12.1658895919

====================
PF (d)  F[?

12.1551873811

====================
Sage Publications, 2012.

11.9791280845

====================
In AutoML, 2014.

11.9262159595

====================
In ICDE, 2013.

11.9142721658

====================
]Ti I(d, q) >= I(d, q 0 ) [?]

11.860776822

====================
Ti |[?

11.860776822

====================
[12] C. C. Cao, J.

11.7400742784

====================
In Management Science.

11.5868409058

====================
Elsevier, 2011.

11.3569463689

====================
In ACL, 2012.

11.2755349698

====================
In ICDE, 2012.

11.2755349698

====================
[46] J. Kim.

11.2691646231

====================
Thus, Precog segments documents at topic-level units.

11.201235532

====================
js Essentials.

10.9943364506

====================
The experiment was IRB approved.

10.9922851678

====================
[79] G. Saito.

10.924745233

====================
In Media, Culture & Society.

10.7970132847

====================
[63] Microsoft.

10.6418077592

====================
[56] S. Loria.

10.552205696

====================
Springer, 2015.

10.5018175499

====================
In EC, 2007.

10.384637069

====================
In WWW, 2010.

10.3280576895

====================
[93] N. World.

10.3138448294

====================
In CIS, 2011.

10.1047612957

====================
Quality evaluation of product reviews using an information quality framework.

10.0554241367

====================
[45] R. Kelly.

9.99824066698

====================
Springer, 2013.

9.93299394325

====================
support.office.com, 2016.

9.74361218457

====================
[33] A. Ghosh.

9.57334284282

====================
In JMLR, 2003.

9.41216738721

====================
The discount function [?]

9.1575390701

====================
[92] Wikipedia.

9.1238701706

====================
In SIGMOD, 2016.

8.98632706435

====================
In CIKM, 2012.

8.93460564682

====================
In EC, 2009.

8.86163704814

====================
RELATED WORK  9.

8.80536075283

====================
Also, let ~e [?]

8.67317077287

====================
MIT Press, 2002.

8.56543556004

====================
In Decision Support Systems.

8.52452131944

====================
In WSDM, 2008.

8.49382537256

====================
In EMNLP-CoNLL, 2007.

8.1187805348

====================
In AAAI, 2004.

8.01908104947

====================
Guy.

7.89915348334

====================
](p) and the model's prediction confidence C(d + p) [?]

7.77477739666

====================
In ACL, 2006.

7.68803685366

====================
Technical report, 2015.

7.65325169878

====================
Get another label?

7.4781520647

====================
In WWW, 2016.

7.46129713233

====================
VLDB, 2011.

7.46129713233

====================
In Communications of the ACM, 2007.

7.32266616069

====================
ACM, 2012.

7.30772271428

====================
In Information Processing & Management.

6.76426249402

====================
Let F be the set of n model features, and fi denote the ith feature.

6.7025493245

====================
Let ~s [?]

6.61274557658

====================
What makes a helpful review?

6.42345988944

====================
In L@S, 2015.

6.38753176855

====================
[10] Boomerang.

6.18826412308

====================
In SNAM.

6.18826412308

====================
React.

6.18826412308

====================
In MindTrek, 2011.

6.18826412308

====================
In RecSys, 2016.

6.18826412308

====================
Whom to ask?

6.18826412308

====================
To the best of our 12  10.

6.06742498713

====================
Machine Learning, 2016.

6.06742498713

====================
[37] I.

5.99919849039

====================
[35] Google.

5.61069150979

====================
]q0 [?

5.45003052745

====================
Springer, 2002.

4.92717355899

====================
Let D = {d1 , .

4.8044568054

====================
, ym } be their labels.

4.80402104473

====================
The first one is how to split a document into segments.

4.55047613764

====================
13  [51] J.

4.48835968454

====================
Supervised topic models.

4.40671924726

====================
For instance, consider [?

3.87020040979

====================
Fox.

3.80666248977

====================
4.

3.04452243772

====================
Figure 5: Precog architecture.

2.72501526372

====================
3.

2.30258509299

====================
2  2.

1.94591014906

====================
In Science.

1.45464719098

====================
IEEE, 2011.

1.09861228867

====================
[24] J.

1.09861228867

====================
"why should I trust you?

0.0

====================
U (M (d + p)) - U (M (d)) x C(d + p) [?

0.0

====================
7  [?

0.0

====================
8.

0.0

====================
for host profiles.

0.0

====================
[2] Amazon.

0.0

====================
14

0.0

====================
", or 0 if the document did not change.

0.0

====================
F to feedback text.

0.0

====================
6.

0.0

====================
]fi [?

0.0

====================
.

0.0

====================
]M q j [?

0.0

====================
5.

0.0

====================
[0, 1].

0.0

====================
]F / (pi = 0) p[?

0.0

====================
[?

0.0

To this end, we present Precog1 , a crowdsourced data acquisition system that supports pre-hoc quality control for both simple data types and multi-paragraph text attributes. We further show that Precog's unique approach to combining prescriptive explanations and segment-level feedback improves text quality by 14.3%, and over 3x better than a state-of-the-art feedback system [50]. To build high-quality models, we survey and categorize features from the writing analysis literature into 5 categories  1. user input  2. segment by topic  3. estimate quality  4. targeted feedback  Figure 3: The Segment-Predict-Explain pattern: Precog splits user input into coherent segments; estimates the quality of each segment and the text as a whole; and generates and shows suggested improvements to the user. The fourth FEF for product reviews was mapped to Subjectivity features in (Table 1) and the fourth host profiles FEF was mapped to Friendliness LIWC features shown in [94], with each returning text suggesting that the user improves the respective facet of their submission (i.e., "Please make your writing more balanced and neutral"). Experimental Conditions: The purpose of experiments is to both show the Cost Saving benefits of Precog as well as to evaluate the effectiveness of it's two main features (segment-level feedback and TCruise explanation generation). NEW APPLICATION DOMAINS  EXPERIMENTS  We now evaluate how Precog improves high-quality data acquisition using live Mechanical Turk deployments. Our experiments and prior work [52] show that this is less effective than a more customized approach. For explanation functions, prior work showed that 75% of reasons for unhelpful reviews were covered by (in priority order) overly emotional/biased opinions, lack of information/not enough detail, irrelevant comments, and poor writing style [18]. In addition to evaluating Precog for hard constraints and simple data types, we evaluate Precog's text feedback through extensive MTurk experiments on two real application domains--product reviews and rental host profiles. Through extensive MTurk experiments, we find that Precog collects >= 2x more high-quality documents and improves text quality by 14.3% compared to not using pre-hoc techniques.
